{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03da1bfe-de5e-4d73-849c-11a08862cbfc",
   "metadata": {},
   "source": [
    "# Chapter X: Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "950c1a8d-e9b1-42a0-bcc7-4bfdc1f454c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD:\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57751b0b-3c72-410d-9de8-3b552b2c6983",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember inputs for backpropagation\n",
    "        self.inputs = inputs\n",
    "        # Output = X·W + b\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        # ∂L/∂W = Xᵀ · dL/dZ\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        # ∂L/∂b = sum of dL/dZ over samples\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # ∂L/∂X = dL/dZ · Wᵀ\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "490a7cd0-664d-4877-89a2-575249b28176",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember inputs for use in backpropagation\n",
    "        self.inputs = inputs\n",
    "        # Output = max(0, x)\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Make a copy of the gradients\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83cd4320-a7ba-4d96-9983-614200279124",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            # Calculate sample-wise gradient and store it\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85637c63-c2b2-4941-b93c-feac9dcfda63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common loss class\n",
    "class Loss:\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        # Return loss\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f157e9ff-8dae-47f4-b3e6-a2d18ba32c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "            range(samples),\n",
    "            y_true\n",
    "            ]\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "            y_pred_clipped * y_true,\n",
    "            axis=1\n",
    "            )\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8d70f94-2e6c-4c70-a014-87a3c1622ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "263c4dbd-313d-4da2-bbdf-5469a07b259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import numpy as np\n",
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f837640-65cb-4b99-9537-64d3a1b0d784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.098592\n",
      "acc: 0.35333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD()\n",
    "# Perform a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "# Perform a forward pass through the activation/loss function\n",
    "# takes the output of second dense layer here and returns loss\n",
    "loss = loss_activation.forward(dense2.output, y)\n",
    "# Let's print loss value\n",
    "print('loss:', loss)\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis=1)\n",
    "accuracy = np.mean(predictions==y)\n",
    "print('acc:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d7631790-df03-4ee8-8e24-fe3a589fdc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass\n",
    "loss_activation.backward(loss_activation.output,\n",
    "                         y)\n",
    "dense2.backward(loss_activation.dinputs)\n",
    "activation1.backward(dense2.dinputs)\n",
    "dense1.backward(activation1.dinputs)\n",
    "# Update weights and biases\n",
    "optimizer.update_params(dense1)\n",
    "optimizer.update_params(dense2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92b732e9-9686-495a-8e2f-efadd64a5ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.317, loss: 1.099\n",
      "epoch: 100, acc: 0.407, loss: 1.091\n",
      "epoch: 200, acc: 0.417, loss: 1.079\n",
      "epoch: 300, acc: 0.423, loss: 1.077\n",
      "epoch: 400, acc: 0.407, loss: 1.074\n",
      "epoch: 500, acc: 0.410, loss: 1.067\n",
      "epoch: 600, acc: 0.443, loss: 1.054\n",
      "epoch: 700, acc: 0.457, loss: 1.041\n",
      "epoch: 800, acc: 0.430, loss: 1.030\n",
      "epoch: 900, acc: 0.423, loss: 1.018\n",
      "epoch: 1000, acc: 0.430, loss: 1.018\n",
      "epoch: 1100, acc: 0.473, loss: 1.000\n",
      "epoch: 1200, acc: 0.493, loss: 0.984\n",
      "epoch: 1300, acc: 0.487, loss: 0.972\n",
      "epoch: 1400, acc: 0.490, loss: 0.960\n",
      "epoch: 1500, acc: 0.493, loss: 0.951\n",
      "epoch: 1600, acc: 0.497, loss: 0.942\n",
      "epoch: 1700, acc: 0.507, loss: 0.931\n",
      "epoch: 1800, acc: 0.510, loss: 0.920\n",
      "epoch: 1900, acc: 0.510, loss: 0.927\n",
      "epoch: 2000, acc: 0.507, loss: 0.905\n",
      "epoch: 2100, acc: 0.517, loss: 0.882\n",
      "epoch: 2200, acc: 0.603, loss: 0.853\n",
      "epoch: 2300, acc: 0.607, loss: 0.832\n",
      "epoch: 2400, acc: 0.600, loss: 0.824\n",
      "epoch: 2500, acc: 0.597, loss: 0.820\n",
      "epoch: 2600, acc: 0.593, loss: 0.803\n",
      "epoch: 2700, acc: 0.617, loss: 0.780\n",
      "epoch: 2800, acc: 0.643, loss: 0.745\n",
      "epoch: 2900, acc: 0.650, loss: 0.718\n",
      "epoch: 3000, acc: 0.630, loss: 0.754\n",
      "epoch: 3100, acc: 0.703, loss: 0.661\n",
      "epoch: 3200, acc: 0.663, loss: 0.694\n",
      "epoch: 3300, acc: 0.637, loss: 0.709\n",
      "epoch: 3400, acc: 0.693, loss: 0.652\n",
      "epoch: 3500, acc: 0.620, loss: 0.802\n",
      "epoch: 3600, acc: 0.673, loss: 0.670\n",
      "epoch: 3700, acc: 0.720, loss: 0.628\n",
      "epoch: 3800, acc: 0.723, loss: 0.620\n",
      "epoch: 3900, acc: 0.510, loss: 0.955\n",
      "epoch: 4000, acc: 0.743, loss: 0.602\n",
      "epoch: 4100, acc: 0.723, loss: 0.612\n",
      "epoch: 4200, acc: 0.737, loss: 0.596\n",
      "epoch: 4300, acc: 0.720, loss: 0.604\n",
      "epoch: 4400, acc: 0.677, loss: 0.667\n",
      "epoch: 4500, acc: 0.620, loss: 0.715\n",
      "epoch: 4600, acc: 0.667, loss: 0.653\n",
      "epoch: 4700, acc: 0.723, loss: 0.588\n",
      "epoch: 4800, acc: 0.730, loss: 0.573\n",
      "epoch: 4900, acc: 0.707, loss: 0.605\n",
      "epoch: 5000, acc: 0.790, loss: 0.552\n",
      "epoch: 5100, acc: 0.710, loss: 0.594\n",
      "epoch: 5200, acc: 0.743, loss: 0.567\n",
      "epoch: 5300, acc: 0.747, loss: 0.534\n",
      "epoch: 5400, acc: 0.730, loss: 0.580\n",
      "epoch: 5500, acc: 0.713, loss: 0.589\n",
      "epoch: 5600, acc: 0.740, loss: 0.567\n",
      "epoch: 5700, acc: 0.737, loss: 0.564\n",
      "epoch: 5800, acc: 0.767, loss: 0.536\n",
      "epoch: 5900, acc: 0.763, loss: 0.539\n",
      "epoch: 6000, acc: 0.560, loss: 1.392\n",
      "epoch: 6100, acc: 0.727, loss: 0.567\n",
      "epoch: 6200, acc: 0.777, loss: 0.536\n",
      "epoch: 6300, acc: 0.797, loss: 0.510\n",
      "epoch: 6400, acc: 0.740, loss: 0.560\n",
      "epoch: 6500, acc: 0.790, loss: 0.532\n",
      "epoch: 6600, acc: 0.797, loss: 0.508\n",
      "epoch: 6700, acc: 0.777, loss: 0.545\n",
      "epoch: 6800, acc: 0.800, loss: 0.525\n",
      "epoch: 6900, acc: 0.730, loss: 0.622\n",
      "epoch: 7000, acc: 0.777, loss: 0.544\n",
      "epoch: 7100, acc: 0.783, loss: 0.522\n",
      "epoch: 7200, acc: 0.737, loss: 0.570\n",
      "epoch: 7300, acc: 0.800, loss: 0.501\n",
      "epoch: 7400, acc: 0.787, loss: 0.516\n",
      "epoch: 7500, acc: 0.797, loss: 0.515\n",
      "epoch: 7600, acc: 0.783, loss: 0.533\n",
      "epoch: 7700, acc: 0.780, loss: 0.531\n",
      "epoch: 7800, acc: 0.793, loss: 0.514\n",
      "epoch: 7900, acc: 0.750, loss: 0.573\n",
      "epoch: 8000, acc: 0.780, loss: 0.538\n",
      "epoch: 8100, acc: 0.810, loss: 0.495\n",
      "epoch: 8200, acc: 0.787, loss: 0.508\n",
      "epoch: 8300, acc: 0.793, loss: 0.514\n",
      "epoch: 8400, acc: 0.677, loss: 0.727\n",
      "epoch: 8500, acc: 0.800, loss: 0.500\n",
      "epoch: 8600, acc: 0.807, loss: 0.507\n",
      "epoch: 8700, acc: 0.790, loss: 0.514\n",
      "epoch: 8800, acc: 0.740, loss: 0.624\n",
      "epoch: 8900, acc: 0.780, loss: 0.504\n",
      "epoch: 9000, acc: 0.807, loss: 0.495\n",
      "epoch: 9100, acc: 0.800, loss: 0.486\n",
      "epoch: 9200, acc: 0.807, loss: 0.484\n",
      "epoch: 9300, acc: 0.800, loss: 0.492\n",
      "epoch: 9400, acc: 0.807, loss: 0.479\n",
      "epoch: 9500, acc: 0.810, loss: 0.483\n",
      "epoch: 9600, acc: 0.820, loss: 0.459\n",
      "epoch: 9700, acc: 0.810, loss: 0.457\n",
      "epoch: 9800, acc: 0.810, loss: 0.460\n",
      "epoch: 9900, acc: 0.810, loss: 0.505\n",
      "epoch: 10000, acc: 0.810, loss: 0.476\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD()\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "        f'acc: {accuracy:.3f}, ' +\n",
    "        f'loss: {loss:.3f}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    # Update weights and biases\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c48fcf22-6d44-4904-875a-adf18db69917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a is true\n"
     ]
    }
   ],
   "source": [
    "a = 0.1\n",
    "\n",
    "if (a):\n",
    "    print('a is true')\n",
    "else:\n",
    "    print('a is false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "def82970-a450-4ccb-a607-a28637e7078f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD:\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1.0, decay=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "\n",
    "    # Call once before any parameter updates:\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31723ebe-d43d-4bc5-9e9f-5f6eed2b02bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.413, loss: 1.099, lr: 1.0\n",
      "epoch: 100, acc: 0.447, loss: 1.064, lr: 0.5025125628140703\n",
      "epoch: 200, acc: 0.470, loss: 1.052, lr: 0.33444816053511706\n",
      "epoch: 300, acc: 0.477, loss: 1.047, lr: 0.2506265664160401\n",
      "epoch: 400, acc: 0.480, loss: 1.042, lr: 0.2004008016032064\n",
      "epoch: 500, acc: 0.470, loss: 1.033, lr: 0.1669449081803005\n",
      "epoch: 600, acc: 0.470, loss: 1.025, lr: 0.14306151645207438\n",
      "epoch: 700, acc: 0.467, loss: 1.011, lr: 0.1251564455569462\n",
      "epoch: 800, acc: 0.490, loss: 0.996, lr: 0.11123470522803114\n",
      "epoch: 900, acc: 0.413, loss: 0.993, lr: 0.10010010010010009\n",
      "epoch: 1000, acc: 0.447, loss: 0.980, lr: 0.09099181073703366\n",
      "epoch: 1100, acc: 0.480, loss: 0.968, lr: 0.08340283569641367\n",
      "epoch: 1200, acc: 0.497, loss: 0.955, lr: 0.07698229407236336\n",
      "epoch: 1300, acc: 0.507, loss: 0.939, lr: 0.07147962830593281\n",
      "epoch: 1400, acc: 0.510, loss: 0.989, lr: 0.066711140760507\n",
      "epoch: 1500, acc: 0.533, loss: 0.934, lr: 0.06253908692933083\n",
      "epoch: 1600, acc: 0.537, loss: 0.929, lr: 0.05885815185403177\n",
      "epoch: 1700, acc: 0.457, loss: 0.914, lr: 0.055586436909394105\n",
      "epoch: 1800, acc: 0.587, loss: 0.921, lr: 0.052659294365455495\n",
      "epoch: 1900, acc: 0.533, loss: 0.939, lr: 0.05002501250625312\n",
      "epoch: 2000, acc: 0.473, loss: 0.910, lr: 0.047641734159123386\n",
      "epoch: 2100, acc: 0.557, loss: 0.904, lr: 0.04547521600727603\n",
      "epoch: 2200, acc: 0.510, loss: 0.948, lr: 0.04349717268377555\n",
      "epoch: 2300, acc: 0.493, loss: 0.909, lr: 0.04168403501458941\n",
      "epoch: 2400, acc: 0.560, loss: 0.897, lr: 0.04001600640256102\n",
      "epoch: 2500, acc: 0.510, loss: 0.892, lr: 0.03847633705271258\n",
      "epoch: 2600, acc: 0.533, loss: 0.888, lr: 0.03705075954057058\n",
      "epoch: 2700, acc: 0.570, loss: 0.905, lr: 0.03572704537334762\n",
      "epoch: 2800, acc: 0.543, loss: 0.873, lr: 0.03449465332873405\n",
      "epoch: 2900, acc: 0.523, loss: 0.878, lr: 0.03334444814938312\n",
      "epoch: 3000, acc: 0.563, loss: 0.867, lr: 0.03226847370119393\n",
      "epoch: 3100, acc: 0.537, loss: 0.884, lr: 0.03125976867771178\n",
      "epoch: 3200, acc: 0.567, loss: 0.846, lr: 0.03031221582297666\n",
      "epoch: 3300, acc: 0.553, loss: 0.835, lr: 0.02942041776993233\n",
      "epoch: 3400, acc: 0.543, loss: 0.841, lr: 0.028579594169762787\n",
      "epoch: 3500, acc: 0.583, loss: 0.838, lr: 0.027785495971103084\n",
      "epoch: 3600, acc: 0.557, loss: 0.869, lr: 0.02703433360367667\n",
      "epoch: 3700, acc: 0.590, loss: 0.807, lr: 0.026322716504343247\n",
      "epoch: 3800, acc: 0.583, loss: 0.836, lr: 0.025647601949217745\n",
      "epoch: 3900, acc: 0.560, loss: 0.885, lr: 0.02500625156289072\n",
      "epoch: 4000, acc: 0.577, loss: 0.836, lr: 0.02439619419370578\n",
      "epoch: 4100, acc: 0.580, loss: 0.819, lr: 0.023815194093831864\n",
      "epoch: 4200, acc: 0.623, loss: 0.786, lr: 0.02326122354035822\n",
      "epoch: 4300, acc: 0.643, loss: 0.806, lr: 0.022732439190725165\n",
      "epoch: 4400, acc: 0.633, loss: 0.780, lr: 0.02222716159146477\n",
      "epoch: 4500, acc: 0.647, loss: 0.771, lr: 0.021743857360295715\n",
      "epoch: 4600, acc: 0.650, loss: 0.775, lr: 0.021281123643328365\n",
      "epoch: 4700, acc: 0.650, loss: 0.780, lr: 0.02083767451552407\n",
      "epoch: 4800, acc: 0.663, loss: 0.772, lr: 0.020412329046744233\n",
      "epoch: 4900, acc: 0.647, loss: 0.768, lr: 0.020004000800160033\n",
      "epoch: 5000, acc: 0.667, loss: 0.761, lr: 0.019611688566385566\n",
      "epoch: 5100, acc: 0.677, loss: 0.746, lr: 0.019234468166955183\n",
      "epoch: 5200, acc: 0.677, loss: 0.744, lr: 0.018871485185884128\n",
      "epoch: 5300, acc: 0.610, loss: 0.834, lr: 0.018521948508983144\n",
      "epoch: 5400, acc: 0.640, loss: 0.722, lr: 0.01818512456810329\n",
      "epoch: 5500, acc: 0.643, loss: 0.724, lr: 0.01786033220217896\n",
      "epoch: 5600, acc: 0.667, loss: 0.742, lr: 0.01754693805930865\n",
      "epoch: 5700, acc: 0.647, loss: 0.778, lr: 0.01724435247456458\n",
      "epoch: 5800, acc: 0.723, loss: 0.739, lr: 0.016952025767079167\n",
      "epoch: 5900, acc: 0.627, loss: 0.864, lr: 0.01666944490748458\n",
      "epoch: 6000, acc: 0.647, loss: 0.732, lr: 0.016396130513198885\n",
      "epoch: 6100, acc: 0.680, loss: 0.730, lr: 0.016131634134537828\n",
      "epoch: 6200, acc: 0.680, loss: 0.777, lr: 0.015875535799333228\n",
      "epoch: 6300, acc: 0.660, loss: 0.710, lr: 0.01562744178777934\n",
      "epoch: 6400, acc: 0.677, loss: 0.769, lr: 0.015386982612709646\n",
      "epoch: 6500, acc: 0.673, loss: 0.729, lr: 0.015153811183512654\n",
      "epoch: 6600, acc: 0.697, loss: 0.734, lr: 0.014927601134497688\n",
      "epoch: 6700, acc: 0.667, loss: 0.751, lr: 0.014708045300779527\n",
      "epoch: 6800, acc: 0.700, loss: 0.713, lr: 0.014494854326714018\n",
      "epoch: 6900, acc: 0.587, loss: 0.840, lr: 0.014287755393627663\n",
      "epoch: 7000, acc: 0.667, loss: 0.695, lr: 0.014086491055078181\n",
      "epoch: 7100, acc: 0.670, loss: 0.735, lr: 0.013890818169190166\n",
      "epoch: 7200, acc: 0.647, loss: 0.782, lr: 0.013700506918755994\n",
      "epoch: 7300, acc: 0.680, loss: 0.746, lr: 0.013515339910798757\n",
      "epoch: 7400, acc: 0.690, loss: 0.724, lr: 0.013335111348179758\n",
      "epoch: 7500, acc: 0.687, loss: 0.715, lr: 0.013159626266614028\n",
      "epoch: 7600, acc: 0.727, loss: 0.701, lr: 0.012988699831146902\n",
      "epoch: 7700, acc: 0.743, loss: 0.679, lr: 0.012822156686754713\n",
      "epoch: 7800, acc: 0.707, loss: 0.707, lr: 0.0126598303582732\n",
      "epoch: 7900, acc: 0.687, loss: 0.726, lr: 0.012501562695336917\n",
      "epoch: 8000, acc: 0.657, loss: 0.757, lr: 0.012347203358439314\n",
      "epoch: 8100, acc: 0.630, loss: 0.840, lr: 0.012196609342602758\n",
      "epoch: 8200, acc: 0.713, loss: 0.655, lr: 0.012049644535486204\n",
      "epoch: 8300, acc: 0.743, loss: 0.569, lr: 0.011906179307060364\n",
      "epoch: 8400, acc: 0.747, loss: 0.583, lr: 0.011766090128250382\n",
      "epoch: 8500, acc: 0.777, loss: 0.539, lr: 0.01162925921618793\n",
      "epoch: 8600, acc: 0.660, loss: 0.843, lr: 0.011495574203931488\n",
      "epoch: 8700, acc: 0.747, loss: 0.559, lr: 0.011364927832708264\n",
      "epoch: 8800, acc: 0.740, loss: 0.576, lr: 0.011237217664906169\n",
      "epoch: 8900, acc: 0.703, loss: 0.707, lr: 0.011112345816201801\n",
      "epoch: 9000, acc: 0.767, loss: 0.601, lr: 0.010990218705352238\n",
      "epoch: 9100, acc: 0.773, loss: 0.584, lr: 0.010870746820306556\n",
      "epoch: 9200, acc: 0.780, loss: 0.517, lr: 0.010753844499408539\n",
      "epoch: 9300, acc: 0.770, loss: 0.604, lr: 0.010639429726566656\n",
      "epoch: 9400, acc: 0.753, loss: 0.574, lr: 0.010527423939362039\n",
      "epoch: 9500, acc: 0.767, loss: 0.597, lr: 0.010417751849150954\n",
      "epoch: 9600, acc: 0.753, loss: 0.651, lr: 0.010310341272296112\n",
      "epoch: 9700, acc: 0.800, loss: 0.506, lr: 0.010205122971731808\n",
      "epoch: 9800, acc: 0.763, loss: 0.606, lr: 0.010102030508132133\n",
      "epoch: 9900, acc: 0.737, loss: 0.593, lr: 0.01000100010001\n",
      "epoch: 10000, acc: 0.760, loss: 0.602, lr: 0.009901970492127933\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD(decay=1e-2)\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "        f'acc: {accuracy:.3f}, ' +\n",
    "        f'loss: {loss:.3f}, ' +\n",
    "        f'lr: {optimizer.current_learning_rate}')\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "faff0382-9878-4978-90d9-2362f0c0694a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.297, loss: 1.099, lr: 1.0\n",
      "epoch: 100, acc: 0.417, loss: 1.079, lr: 0.9099181073703367\n",
      "epoch: 200, acc: 0.397, loss: 1.069, lr: 0.8340283569641367\n",
      "epoch: 300, acc: 0.403, loss: 1.067, lr: 0.7698229407236336\n",
      "epoch: 400, acc: 0.410, loss: 1.065, lr: 0.7147962830593281\n",
      "epoch: 500, acc: 0.413, loss: 1.064, lr: 0.66711140760507\n",
      "epoch: 600, acc: 0.413, loss: 1.061, lr: 0.6253908692933083\n",
      "epoch: 700, acc: 0.430, loss: 1.055, lr: 0.5885815185403178\n",
      "epoch: 800, acc: 0.447, loss: 1.050, lr: 0.5558643690939411\n",
      "epoch: 900, acc: 0.410, loss: 1.058, lr: 0.526592943654555\n",
      "epoch: 1000, acc: 0.417, loss: 1.056, lr: 0.5002501250625312\n",
      "epoch: 1100, acc: 0.400, loss: 1.056, lr: 0.4764173415912339\n",
      "epoch: 1200, acc: 0.393, loss: 1.053, lr: 0.45475216007276037\n",
      "epoch: 1300, acc: 0.403, loss: 1.050, lr: 0.43497172683775553\n",
      "epoch: 1400, acc: 0.403, loss: 1.045, lr: 0.4168403501458941\n",
      "epoch: 1500, acc: 0.397, loss: 1.039, lr: 0.4001600640256102\n",
      "epoch: 1600, acc: 0.417, loss: 1.038, lr: 0.3847633705271258\n",
      "epoch: 1700, acc: 0.410, loss: 1.070, lr: 0.3705075954057058\n",
      "epoch: 1800, acc: 0.400, loss: 1.042, lr: 0.35727045373347627\n",
      "epoch: 1900, acc: 0.447, loss: 1.028, lr: 0.3449465332873405\n",
      "epoch: 2000, acc: 0.423, loss: 1.034, lr: 0.33344448149383127\n",
      "epoch: 2100, acc: 0.467, loss: 1.023, lr: 0.32268473701193934\n",
      "epoch: 2200, acc: 0.430, loss: 1.015, lr: 0.31259768677711786\n",
      "epoch: 2300, acc: 0.470, loss: 1.010, lr: 0.3031221582297666\n",
      "epoch: 2400, acc: 0.443, loss: 1.009, lr: 0.29420417769932333\n",
      "epoch: 2500, acc: 0.470, loss: 0.997, lr: 0.2857959416976279\n",
      "epoch: 2600, acc: 0.437, loss: 1.001, lr: 0.2778549597110308\n",
      "epoch: 2700, acc: 0.467, loss: 0.991, lr: 0.2703433360367667\n",
      "epoch: 2800, acc: 0.480, loss: 1.007, lr: 0.26322716504343247\n",
      "epoch: 2900, acc: 0.483, loss: 0.972, lr: 0.25647601949217746\n",
      "epoch: 3000, acc: 0.527, loss: 0.949, lr: 0.25006251562890724\n",
      "epoch: 3100, acc: 0.497, loss: 0.948, lr: 0.2439619419370578\n",
      "epoch: 3200, acc: 0.510, loss: 0.918, lr: 0.23815194093831865\n",
      "epoch: 3300, acc: 0.487, loss: 0.923, lr: 0.23261223540358225\n",
      "epoch: 3400, acc: 0.517, loss: 0.935, lr: 0.22732439190725165\n",
      "epoch: 3500, acc: 0.527, loss: 0.892, lr: 0.22227161591464767\n",
      "epoch: 3600, acc: 0.517, loss: 0.930, lr: 0.21743857360295715\n",
      "epoch: 3700, acc: 0.547, loss: 0.887, lr: 0.21281123643328367\n",
      "epoch: 3800, acc: 0.487, loss: 0.964, lr: 0.20837674515524068\n",
      "epoch: 3900, acc: 0.537, loss: 0.878, lr: 0.20412329046744235\n",
      "epoch: 4000, acc: 0.533, loss: 0.906, lr: 0.2000400080016003\n",
      "epoch: 4100, acc: 0.507, loss: 1.002, lr: 0.19611688566385566\n",
      "epoch: 4200, acc: 0.533, loss: 0.918, lr: 0.19234468166955185\n",
      "epoch: 4300, acc: 0.567, loss: 0.861, lr: 0.18871485185884126\n",
      "epoch: 4400, acc: 0.560, loss: 0.883, lr: 0.18521948508983144\n",
      "epoch: 4500, acc: 0.587, loss: 0.843, lr: 0.18185124568103292\n",
      "epoch: 4600, acc: 0.567, loss: 0.859, lr: 0.1786033220217896\n",
      "epoch: 4700, acc: 0.593, loss: 0.834, lr: 0.1754693805930865\n",
      "epoch: 4800, acc: 0.607, loss: 0.825, lr: 0.17244352474564578\n",
      "epoch: 4900, acc: 0.607, loss: 0.821, lr: 0.16952025767079165\n",
      "epoch: 5000, acc: 0.587, loss: 0.843, lr: 0.16669444907484582\n",
      "epoch: 5100, acc: 0.527, loss: 0.877, lr: 0.16396130513198884\n",
      "epoch: 5200, acc: 0.620, loss: 0.833, lr: 0.16131634134537828\n",
      "epoch: 5300, acc: 0.590, loss: 0.830, lr: 0.15875535799333226\n",
      "epoch: 5400, acc: 0.627, loss: 0.795, lr: 0.1562744178777934\n",
      "epoch: 5500, acc: 0.617, loss: 0.827, lr: 0.15386982612709646\n",
      "epoch: 5600, acc: 0.563, loss: 0.879, lr: 0.15153811183512653\n",
      "epoch: 5700, acc: 0.633, loss: 0.830, lr: 0.14927601134497687\n",
      "epoch: 5800, acc: 0.587, loss: 0.819, lr: 0.14708045300779526\n",
      "epoch: 5900, acc: 0.617, loss: 0.830, lr: 0.14494854326714016\n",
      "epoch: 6000, acc: 0.580, loss: 0.817, lr: 0.1428775539362766\n",
      "epoch: 6100, acc: 0.643, loss: 0.834, lr: 0.1408649105507818\n",
      "epoch: 6200, acc: 0.587, loss: 0.825, lr: 0.13890818169190167\n",
      "epoch: 6300, acc: 0.613, loss: 0.829, lr: 0.13700506918755992\n",
      "epoch: 6400, acc: 0.627, loss: 0.823, lr: 0.13515339910798757\n",
      "epoch: 6500, acc: 0.593, loss: 0.803, lr: 0.13335111348179757\n",
      "epoch: 6600, acc: 0.630, loss: 0.801, lr: 0.13159626266614027\n",
      "epoch: 6700, acc: 0.640, loss: 0.795, lr: 0.12988699831146902\n",
      "epoch: 6800, acc: 0.623, loss: 0.774, lr: 0.12822156686754713\n",
      "epoch: 6900, acc: 0.520, loss: 1.040, lr: 0.126598303582732\n",
      "epoch: 7000, acc: 0.637, loss: 0.756, lr: 0.12501562695336915\n",
      "epoch: 7100, acc: 0.613, loss: 0.824, lr: 0.12347203358439313\n",
      "epoch: 7200, acc: 0.677, loss: 0.714, lr: 0.12196609342602757\n",
      "epoch: 7300, acc: 0.647, loss: 0.741, lr: 0.12049644535486204\n",
      "epoch: 7400, acc: 0.620, loss: 0.758, lr: 0.11906179307060363\n",
      "epoch: 7500, acc: 0.620, loss: 0.790, lr: 0.11766090128250381\n",
      "epoch: 7600, acc: 0.613, loss: 0.809, lr: 0.11629259216187929\n",
      "epoch: 7700, acc: 0.613, loss: 0.784, lr: 0.11495574203931487\n",
      "epoch: 7800, acc: 0.497, loss: 1.662, lr: 0.11364927832708263\n",
      "epoch: 7900, acc: 0.683, loss: 0.683, lr: 0.11237217664906168\n",
      "epoch: 8000, acc: 0.640, loss: 0.720, lr: 0.11112345816201799\n",
      "epoch: 8100, acc: 0.650, loss: 0.692, lr: 0.10990218705352237\n",
      "epoch: 8200, acc: 0.667, loss: 0.690, lr: 0.10870746820306555\n",
      "epoch: 8300, acc: 0.673, loss: 0.685, lr: 0.1075384449940854\n",
      "epoch: 8400, acc: 0.663, loss: 0.728, lr: 0.10639429726566654\n",
      "epoch: 8500, acc: 0.670, loss: 0.701, lr: 0.10527423939362038\n",
      "epoch: 8600, acc: 0.660, loss: 0.713, lr: 0.10417751849150952\n",
      "epoch: 8700, acc: 0.673, loss: 0.698, lr: 0.10310341272296113\n",
      "epoch: 8800, acc: 0.487, loss: 1.131, lr: 0.1020512297173181\n",
      "epoch: 8900, acc: 0.683, loss: 0.648, lr: 0.10102030508132134\n",
      "epoch: 9000, acc: 0.650, loss: 0.671, lr: 0.1000100010001\n",
      "epoch: 9100, acc: 0.690, loss: 0.640, lr: 0.09901970492127933\n",
      "epoch: 9200, acc: 0.710, loss: 0.612, lr: 0.09804882831650162\n",
      "epoch: 9300, acc: 0.673, loss: 0.650, lr: 0.09709680551509856\n",
      "epoch: 9400, acc: 0.690, loss: 0.632, lr: 0.09616309260505818\n",
      "epoch: 9500, acc: 0.603, loss: 0.919, lr: 0.09524716639679968\n",
      "epoch: 9600, acc: 0.680, loss: 0.636, lr: 0.09434852344560807\n",
      "epoch: 9700, acc: 0.683, loss: 0.648, lr: 0.09346667912889055\n",
      "epoch: 9800, acc: 0.680, loss: 0.629, lr: 0.09260116677470137\n",
      "epoch: 9900, acc: 0.663, loss: 0.669, lr: 0.09175153683824203\n",
      "epoch: 10000, acc: 0.677, loss: 0.634, lr: 0.09091735612328393\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD(decay=1e-3)\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "        f'acc: {accuracy:.3f}, ' +\n",
    "        f'loss: {loss:.3f}, ' +\n",
    "        f'lr: {optimizer.current_learning_rate}')\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "952e1eca-9e0c-4371-9f3f-c4aaf8685735",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD:\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1.0, decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    # Call once before any parameter updates:\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "            # If layer does not contain momentum arrays, create them\n",
    "            # filled with zeros\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                # If there is no momentum array for weights\n",
    "                # The array doesn't exist for biases yet either.\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "    \n",
    "            # Build bias updates\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        # Vanilla SGD updates (as before momentum update)\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "        # Update weights and biases using either\n",
    "        # vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a37902f7-e3a5-4785-86b1-39ef3604c244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.327, loss: 1.099, lr: 1.0\n",
      "epoch: 100, acc: 0.430, loss: 1.075, lr: 0.9099181073703367\n",
      "epoch: 200, acc: 0.423, loss: 1.070, lr: 0.8340283569641367\n",
      "epoch: 300, acc: 0.420, loss: 1.065, lr: 0.7698229407236336\n",
      "epoch: 400, acc: 0.437, loss: 1.052, lr: 0.7147962830593281\n",
      "epoch: 500, acc: 0.433, loss: 1.030, lr: 0.66711140760507\n",
      "epoch: 600, acc: 0.473, loss: 1.000, lr: 0.6253908692933083\n",
      "epoch: 700, acc: 0.480, loss: 0.999, lr: 0.5885815185403178\n",
      "epoch: 800, acc: 0.510, loss: 0.986, lr: 0.5558643690939411\n",
      "epoch: 900, acc: 0.463, loss: 0.971, lr: 0.526592943654555\n",
      "epoch: 1000, acc: 0.470, loss: 0.962, lr: 0.5002501250625312\n",
      "epoch: 1100, acc: 0.483, loss: 0.941, lr: 0.4764173415912339\n",
      "epoch: 1200, acc: 0.527, loss: 0.918, lr: 0.45475216007276037\n",
      "epoch: 1300, acc: 0.547, loss: 0.900, lr: 0.43497172683775553\n",
      "epoch: 1400, acc: 0.537, loss: 0.883, lr: 0.4168403501458941\n",
      "epoch: 1500, acc: 0.540, loss: 0.869, lr: 0.4001600640256102\n",
      "epoch: 1600, acc: 0.560, loss: 0.848, lr: 0.3847633705271258\n",
      "epoch: 1700, acc: 0.587, loss: 0.844, lr: 0.3705075954057058\n",
      "epoch: 1800, acc: 0.587, loss: 0.826, lr: 0.35727045373347627\n",
      "epoch: 1900, acc: 0.597, loss: 0.818, lr: 0.3449465332873405\n",
      "epoch: 2000, acc: 0.607, loss: 0.810, lr: 0.33344448149383127\n",
      "epoch: 2100, acc: 0.617, loss: 0.799, lr: 0.32268473701193934\n",
      "epoch: 2200, acc: 0.620, loss: 0.795, lr: 0.31259768677711786\n",
      "epoch: 2300, acc: 0.637, loss: 0.783, lr: 0.3031221582297666\n",
      "epoch: 2400, acc: 0.657, loss: 0.780, lr: 0.29420417769932333\n",
      "epoch: 2500, acc: 0.650, loss: 0.772, lr: 0.2857959416976279\n",
      "epoch: 2600, acc: 0.657, loss: 0.766, lr: 0.2778549597110308\n",
      "epoch: 2700, acc: 0.657, loss: 0.755, lr: 0.2703433360367667\n",
      "epoch: 2800, acc: 0.660, loss: 0.753, lr: 0.26322716504343247\n",
      "epoch: 2900, acc: 0.653, loss: 0.748, lr: 0.25647601949217746\n",
      "epoch: 3000, acc: 0.650, loss: 0.743, lr: 0.25006251562890724\n",
      "epoch: 3100, acc: 0.660, loss: 0.736, lr: 0.2439619419370578\n",
      "epoch: 3200, acc: 0.663, loss: 0.726, lr: 0.23815194093831865\n",
      "epoch: 3300, acc: 0.673, loss: 0.715, lr: 0.23261223540358225\n",
      "epoch: 3400, acc: 0.667, loss: 0.710, lr: 0.22732439190725165\n",
      "epoch: 3500, acc: 0.667, loss: 0.704, lr: 0.22227161591464767\n",
      "epoch: 3600, acc: 0.667, loss: 0.703, lr: 0.21743857360295715\n",
      "epoch: 3700, acc: 0.670, loss: 0.689, lr: 0.21281123643328367\n",
      "epoch: 3800, acc: 0.677, loss: 0.681, lr: 0.20837674515524068\n",
      "epoch: 3900, acc: 0.680, loss: 0.672, lr: 0.20412329046744235\n",
      "epoch: 4000, acc: 0.683, loss: 0.663, lr: 0.2000400080016003\n",
      "epoch: 4100, acc: 0.697, loss: 0.654, lr: 0.19611688566385566\n",
      "epoch: 4200, acc: 0.700, loss: 0.645, lr: 0.19234468166955185\n",
      "epoch: 4300, acc: 0.703, loss: 0.638, lr: 0.18871485185884126\n",
      "epoch: 4400, acc: 0.717, loss: 0.628, lr: 0.18521948508983144\n",
      "epoch: 4500, acc: 0.710, loss: 0.628, lr: 0.18185124568103292\n",
      "epoch: 4600, acc: 0.727, loss: 0.614, lr: 0.1786033220217896\n",
      "epoch: 4700, acc: 0.727, loss: 0.603, lr: 0.1754693805930865\n",
      "epoch: 4800, acc: 0.743, loss: 0.593, lr: 0.17244352474564578\n",
      "epoch: 4900, acc: 0.753, loss: 0.585, lr: 0.16952025767079165\n",
      "epoch: 5000, acc: 0.763, loss: 0.575, lr: 0.16669444907484582\n",
      "epoch: 5100, acc: 0.763, loss: 0.569, lr: 0.16396130513198884\n",
      "epoch: 5200, acc: 0.773, loss: 0.556, lr: 0.16131634134537828\n",
      "epoch: 5300, acc: 0.773, loss: 0.550, lr: 0.15875535799333226\n",
      "epoch: 5400, acc: 0.780, loss: 0.543, lr: 0.1562744178777934\n",
      "epoch: 5500, acc: 0.790, loss: 0.534, lr: 0.15386982612709646\n",
      "epoch: 5600, acc: 0.797, loss: 0.527, lr: 0.15153811183512653\n",
      "epoch: 5700, acc: 0.807, loss: 0.519, lr: 0.14927601134497687\n",
      "epoch: 5800, acc: 0.820, loss: 0.512, lr: 0.14708045300779526\n",
      "epoch: 5900, acc: 0.820, loss: 0.509, lr: 0.14494854326714016\n",
      "epoch: 6000, acc: 0.823, loss: 0.501, lr: 0.1428775539362766\n",
      "epoch: 6100, acc: 0.827, loss: 0.500, lr: 0.1408649105507818\n",
      "epoch: 6200, acc: 0.833, loss: 0.493, lr: 0.13890818169190167\n",
      "epoch: 6300, acc: 0.833, loss: 0.490, lr: 0.13700506918755992\n",
      "epoch: 6400, acc: 0.830, loss: 0.485, lr: 0.13515339910798757\n",
      "epoch: 6500, acc: 0.837, loss: 0.478, lr: 0.13335111348179757\n",
      "epoch: 6600, acc: 0.837, loss: 0.472, lr: 0.13159626266614027\n",
      "epoch: 6700, acc: 0.837, loss: 0.467, lr: 0.12988699831146902\n",
      "epoch: 6800, acc: 0.840, loss: 0.463, lr: 0.12822156686754713\n",
      "epoch: 6900, acc: 0.843, loss: 0.458, lr: 0.126598303582732\n",
      "epoch: 7000, acc: 0.847, loss: 0.455, lr: 0.12501562695336915\n",
      "epoch: 7100, acc: 0.840, loss: 0.452, lr: 0.12347203358439313\n",
      "epoch: 7200, acc: 0.840, loss: 0.449, lr: 0.12196609342602757\n",
      "epoch: 7300, acc: 0.847, loss: 0.445, lr: 0.12049644535486204\n",
      "epoch: 7400, acc: 0.847, loss: 0.443, lr: 0.11906179307060363\n",
      "epoch: 7500, acc: 0.853, loss: 0.440, lr: 0.11766090128250381\n",
      "epoch: 7600, acc: 0.853, loss: 0.437, lr: 0.11629259216187929\n",
      "epoch: 7700, acc: 0.860, loss: 0.434, lr: 0.11495574203931487\n",
      "epoch: 7800, acc: 0.863, loss: 0.432, lr: 0.11364927832708263\n",
      "epoch: 7900, acc: 0.860, loss: 0.430, lr: 0.11237217664906168\n",
      "epoch: 8000, acc: 0.863, loss: 0.428, lr: 0.11112345816201799\n",
      "epoch: 8100, acc: 0.867, loss: 0.426, lr: 0.10990218705352237\n",
      "epoch: 8200, acc: 0.867, loss: 0.424, lr: 0.10870746820306555\n",
      "epoch: 8300, acc: 0.863, loss: 0.422, lr: 0.1075384449940854\n",
      "epoch: 8400, acc: 0.863, loss: 0.420, lr: 0.10639429726566654\n",
      "epoch: 8500, acc: 0.867, loss: 0.418, lr: 0.10527423939362038\n",
      "epoch: 8600, acc: 0.867, loss: 0.416, lr: 0.10417751849150952\n",
      "epoch: 8700, acc: 0.863, loss: 0.414, lr: 0.10310341272296113\n",
      "epoch: 8800, acc: 0.870, loss: 0.413, lr: 0.1020512297173181\n",
      "epoch: 8900, acc: 0.870, loss: 0.411, lr: 0.10102030508132134\n",
      "epoch: 9000, acc: 0.867, loss: 0.410, lr: 0.1000100010001\n",
      "epoch: 9100, acc: 0.863, loss: 0.408, lr: 0.09901970492127933\n",
      "epoch: 9200, acc: 0.867, loss: 0.407, lr: 0.09804882831650162\n",
      "epoch: 9300, acc: 0.867, loss: 0.405, lr: 0.09709680551509856\n",
      "epoch: 9400, acc: 0.863, loss: 0.404, lr: 0.09616309260505818\n",
      "epoch: 9500, acc: 0.867, loss: 0.403, lr: 0.09524716639679968\n",
      "epoch: 9600, acc: 0.867, loss: 0.401, lr: 0.09434852344560807\n",
      "epoch: 9700, acc: 0.867, loss: 0.400, lr: 0.09346667912889055\n",
      "epoch: 9800, acc: 0.870, loss: 0.399, lr: 0.09260116677470137\n",
      "epoch: 9900, acc: 0.870, loss: 0.398, lr: 0.09175153683824203\n",
      "epoch: 10000, acc: 0.870, loss: 0.397, lr: 0.09091735612328393\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD(decay=1e-3, momentum=0.5)\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "        f'acc: {accuracy:.3f}, ' +\n",
    "        f'loss: {loss:.3f}, ' +\n",
    "        f'lr: {optimizer.current_learning_rate}')\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9987df46-2721-43f2-ac2a-d3fea57f5974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adagrad Optimizer\n",
    "class Optimizer_Adagrad:\n",
    "    # Intialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=1.0, decay=0., epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "        layer.dweights / \\\n",
    "        (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "        layer.dbiases / \\\n",
    "        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "93f48fd4-63ab-4465-b498-df76e49b64e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.343, loss: 1.099, lr: 1.0\n",
      "epoch: 100, acc: 0.543, loss: 0.946, lr: 0.9901970492127933\n",
      "epoch: 200, acc: 0.563, loss: 0.904, lr: 0.9804882831650161\n",
      "epoch: 300, acc: 0.590, loss: 0.856, lr: 0.9709680551509855\n",
      "epoch: 400, acc: 0.597, loss: 0.804, lr: 0.9616309260505818\n",
      "epoch: 500, acc: 0.597, loss: 0.781, lr: 0.9524716639679969\n",
      "epoch: 600, acc: 0.637, loss: 0.751, lr: 0.9434852344560807\n",
      "epoch: 700, acc: 0.670, loss: 0.705, lr: 0.9346667912889054\n",
      "epoch: 800, acc: 0.660, loss: 0.683, lr: 0.9260116677470135\n",
      "epoch: 900, acc: 0.687, loss: 0.663, lr: 0.9175153683824203\n",
      "epoch: 1000, acc: 0.687, loss: 0.648, lr: 0.9091735612328392\n",
      "epoch: 1100, acc: 0.693, loss: 0.639, lr: 0.9009820704567978\n",
      "epoch: 1200, acc: 0.687, loss: 0.628, lr: 0.892936869363336\n",
      "epoch: 1300, acc: 0.680, loss: 0.615, lr: 0.8850340738118416\n",
      "epoch: 1400, acc: 0.690, loss: 0.613, lr: 0.8772699359592947\n",
      "epoch: 1500, acc: 0.687, loss: 0.603, lr: 0.8696408383337683\n",
      "epoch: 1600, acc: 0.677, loss: 0.596, lr: 0.8621432882145013\n",
      "epoch: 1700, acc: 0.693, loss: 0.591, lr: 0.8547739123001966\n",
      "epoch: 1800, acc: 0.683, loss: 0.586, lr: 0.8475294516484448\n",
      "epoch: 1900, acc: 0.700, loss: 0.580, lr: 0.8404067568703253\n",
      "epoch: 2000, acc: 0.697, loss: 0.576, lr: 0.8334027835652972\n",
      "epoch: 2100, acc: 0.703, loss: 0.573, lr: 0.8265145879824779\n",
      "epoch: 2200, acc: 0.703, loss: 0.569, lr: 0.8197393228953193\n",
      "epoch: 2300, acc: 0.703, loss: 0.565, lr: 0.8130742336775347\n",
      "epoch: 2400, acc: 0.703, loss: 0.561, lr: 0.8065166545689169\n",
      "epoch: 2500, acc: 0.703, loss: 0.558, lr: 0.8000640051204096\n",
      "epoch: 2600, acc: 0.710, loss: 0.554, lr: 0.7937137868084768\n",
      "epoch: 2700, acc: 0.713, loss: 0.552, lr: 0.7874635798094338\n",
      "epoch: 2800, acc: 0.717, loss: 0.550, lr: 0.7813110399249941\n",
      "epoch: 2900, acc: 0.717, loss: 0.546, lr: 0.7752538956508256\n",
      "epoch: 3000, acc: 0.710, loss: 0.544, lr: 0.7692899453804138\n",
      "epoch: 3100, acc: 0.710, loss: 0.540, lr: 0.7634170547370028\n",
      "epoch: 3200, acc: 0.707, loss: 0.540, lr: 0.7576331540268202\n",
      "epoch: 3300, acc: 0.707, loss: 0.537, lr: 0.7519362358072035\n",
      "epoch: 3400, acc: 0.710, loss: 0.536, lr: 0.7463243525636241\n",
      "epoch: 3500, acc: 0.710, loss: 0.534, lr: 0.7407956144899621\n",
      "epoch: 3600, acc: 0.717, loss: 0.532, lr: 0.735348187366718\n",
      "epoch: 3700, acc: 0.723, loss: 0.530, lr: 0.7299802905321557\n",
      "epoch: 3800, acc: 0.723, loss: 0.529, lr: 0.7246901949416624\n",
      "epoch: 3900, acc: 0.723, loss: 0.527, lr: 0.7194762213108857\n",
      "epoch: 4000, acc: 0.723, loss: 0.526, lr: 0.7143367383384527\n",
      "epoch: 4100, acc: 0.727, loss: 0.525, lr: 0.7092701610043266\n",
      "epoch: 4200, acc: 0.733, loss: 0.522, lr: 0.7042749489400663\n",
      "epoch: 4300, acc: 0.733, loss: 0.521, lr: 0.6993496048674733\n",
      "epoch: 4400, acc: 0.733, loss: 0.521, lr: 0.6944926731022988\n",
      "epoch: 4500, acc: 0.730, loss: 0.518, lr: 0.6897027381198704\n",
      "epoch: 4600, acc: 0.737, loss: 0.518, lr: 0.6849784231796698\n",
      "epoch: 4700, acc: 0.730, loss: 0.517, lr: 0.6803183890060548\n",
      "epoch: 4800, acc: 0.730, loss: 0.516, lr: 0.6757213325224677\n",
      "epoch: 4900, acc: 0.730, loss: 0.515, lr: 0.6711859856366199\n",
      "epoch: 5000, acc: 0.730, loss: 0.514, lr: 0.6667111140742716\n",
      "epoch: 5100, acc: 0.730, loss: 0.513, lr: 0.6622955162593549\n",
      "epoch: 5200, acc: 0.737, loss: 0.512, lr: 0.6579380222383051\n",
      "epoch: 5300, acc: 0.733, loss: 0.511, lr: 0.6536374926465782\n",
      "epoch: 5400, acc: 0.743, loss: 0.511, lr: 0.649392817715436\n",
      "epoch: 5500, acc: 0.747, loss: 0.509, lr: 0.6452029163171817\n",
      "epoch: 5600, acc: 0.743, loss: 0.508, lr: 0.6410667350471184\n",
      "epoch: 5700, acc: 0.750, loss: 0.507, lr: 0.6369832473405949\n",
      "epoch: 5800, acc: 0.750, loss: 0.506, lr: 0.6329514526235838\n",
      "epoch: 5900, acc: 0.750, loss: 0.505, lr: 0.6289703754953141\n",
      "epoch: 6000, acc: 0.747, loss: 0.505, lr: 0.6250390649415589\n",
      "epoch: 6100, acc: 0.743, loss: 0.504, lr: 0.6211565935772407\n",
      "epoch: 6200, acc: 0.743, loss: 0.503, lr: 0.6173220569170937\n",
      "epoch: 6300, acc: 0.750, loss: 0.502, lr: 0.6135345726731701\n",
      "epoch: 6400, acc: 0.753, loss: 0.500, lr: 0.6097932800780536\n",
      "epoch: 6500, acc: 0.750, loss: 0.500, lr: 0.6060973392326807\n",
      "epoch: 6600, acc: 0.750, loss: 0.499, lr: 0.6024459304777396\n",
      "epoch: 6700, acc: 0.743, loss: 0.499, lr: 0.5988382537876519\n",
      "epoch: 6800, acc: 0.750, loss: 0.498, lr: 0.5952735281862016\n",
      "epoch: 6900, acc: 0.750, loss: 0.497, lr: 0.5917509911829102\n",
      "epoch: 7000, acc: 0.753, loss: 0.496, lr: 0.5882698982293076\n",
      "epoch: 7100, acc: 0.753, loss: 0.496, lr: 0.5848295221942803\n",
      "epoch: 7200, acc: 0.750, loss: 0.495, lr: 0.5814291528577243\n",
      "epoch: 7300, acc: 0.753, loss: 0.494, lr: 0.5780680964217585\n",
      "epoch: 7400, acc: 0.747, loss: 0.494, lr: 0.5747456750387954\n",
      "epoch: 7500, acc: 0.753, loss: 0.492, lr: 0.5714612263557918\n",
      "epoch: 7600, acc: 0.747, loss: 0.492, lr: 0.5682141030740383\n",
      "epoch: 7700, acc: 0.743, loss: 0.491, lr: 0.5650036725238714\n",
      "epoch: 7800, acc: 0.753, loss: 0.489, lr: 0.5618293162537221\n",
      "epoch: 7900, acc: 0.753, loss: 0.488, lr: 0.5586904296329404\n",
      "epoch: 8000, acc: 0.753, loss: 0.487, lr: 0.5555864214678593\n",
      "epoch: 8100, acc: 0.747, loss: 0.488, lr: 0.5525167136305873\n",
      "epoch: 8200, acc: 0.753, loss: 0.486, lr: 0.5494807407000385\n",
      "epoch: 8300, acc: 0.753, loss: 0.486, lr: 0.5464779496147331\n",
      "epoch: 8400, acc: 0.753, loss: 0.485, lr: 0.5435077993369205\n",
      "epoch: 8500, acc: 0.757, loss: 0.484, lr: 0.5405697605275961\n",
      "epoch: 8600, acc: 0.757, loss: 0.483, lr: 0.5376633152320017\n",
      "epoch: 8700, acc: 0.753, loss: 0.483, lr: 0.5347879565752179\n",
      "epoch: 8800, acc: 0.753, loss: 0.481, lr: 0.5319431884674717\n",
      "epoch: 8900, acc: 0.753, loss: 0.480, lr: 0.5291285253188\n",
      "epoch: 9000, acc: 0.760, loss: 0.479, lr: 0.5263434917627243\n",
      "epoch: 9100, acc: 0.760, loss: 0.478, lr: 0.5235876223886068\n",
      "epoch: 9200, acc: 0.760, loss: 0.477, lr: 0.5208604614823689\n",
      "epoch: 9300, acc: 0.760, loss: 0.476, lr: 0.5181615627752734\n",
      "epoch: 9400, acc: 0.760, loss: 0.476, lr: 0.5154904892004742\n",
      "epoch: 9500, acc: 0.757, loss: 0.475, lr: 0.5128468126570593\n",
      "epoch: 9600, acc: 0.760, loss: 0.473, lr: 0.5102301137813153\n",
      "epoch: 9700, acc: 0.760, loss: 0.472, lr: 0.5076399817249606\n",
      "epoch: 9800, acc: 0.757, loss: 0.472, lr: 0.5050760139400979\n",
      "epoch: 9900, acc: 0.760, loss: 0.471, lr: 0.5025378159706518\n",
      "epoch: 10000, acc: 0.757, loss: 0.470, lr: 0.5000250012500626\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "# Create optimizer\n",
    "#optimizer = Optimizer_SGD(decay=8e-8, momentum=0.9)\n",
    "optimizer = Optimizer_Adagrad(decay=1e-4)\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "        f'acc: {accuracy:.3f}, ' +\n",
    "        f'loss: {loss:.3f}, ' +\n",
    "        f'lr: {optimizer.current_learning_rate}')\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "514d33f8-3507-4195-bcd4-243d60722ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSprop optimizer\n",
    "class Optimizer_RMSprop:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + \\\n",
    "            (1 - self.rho) * layer.dweights**2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + \\\n",
    "            (1 - self.rho) * layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "            layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "            layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9e954d96-8e64-4cfe-8190-48b3970199f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer\n",
    "class Optimizer_Adam:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
    "                 beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update momentum with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + \\\n",
    "            (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + \\\n",
    "            (1 - self.beta_1) * layer.dbiases\n",
    "\n",
    "        # Get corrected momentum\n",
    "        # self.iterations is 0 at first pass, start with 1 here\n",
    "        weight_momentums_corrected = layer.weight_momentums / \\\n",
    "            (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / \\\n",
    "            (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
    "            (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
    "            (1 - self.beta_2) * layer.dbiases**2\n",
    "\n",
    "        # Get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / \\\n",
    "            (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / \\\n",
    "            (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "            weight_momentums_corrected / \\\n",
    "            (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "            bias_momentums_corrected / \\\n",
    "            (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
