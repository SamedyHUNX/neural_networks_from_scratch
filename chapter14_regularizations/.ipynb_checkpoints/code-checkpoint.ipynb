{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ab0995e-aa0f-4563-8635-ab0de5fbb811",
   "metadata": {},
   "source": [
    "# Chapter XIV: Regularizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce558cd5-6d92-46d8-b17d-24febdb811b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import numpy as np\n",
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "807a6fbf-76a8-4eab-bbc7-e326c9608d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "# Go up one directory and into 'notebooks'\n",
    "sys.path.append(os.path.abspath('../modules'))\n",
    "\n",
    "from layers import Layer_Dense, Layer_Dropout\n",
    "from activation_functions import Activation_ReLU\n",
    "from losses import Activation_Softmax_Loss_CategoricalCrossentropy\n",
    "from optimizers import Optimizer_Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fe03747-c17e-49c9-bcd9-4645ff586de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.360, loss: 1.099 (data_loss: 1.099, reg_loss: 0.000), lr: 0.02\n",
      "epoch: 100, acc: 0.690, loss: 0.829 (data_loss: 0.787, reg_loss: 0.042), lr: 0.019999010049002574\n",
      "epoch: 200, acc: 0.783, loss: 0.681 (data_loss: 0.602, reg_loss: 0.079), lr: 0.019998010197985302\n",
      "epoch: 300, acc: 0.817, loss: 0.601 (data_loss: 0.507, reg_loss: 0.094), lr: 0.019997010446938183\n",
      "epoch: 400, acc: 0.830, loss: 0.552 (data_loss: 0.449, reg_loss: 0.102), lr: 0.01999601079584623\n",
      "epoch: 500, acc: 0.863, loss: 0.517 (data_loss: 0.411, reg_loss: 0.106), lr: 0.01999501124469445\n",
      "epoch: 600, acc: 0.857, loss: 0.488 (data_loss: 0.381, reg_loss: 0.108), lr: 0.01999401179346786\n",
      "epoch: 700, acc: 0.873, loss: 0.469 (data_loss: 0.361, reg_loss: 0.108), lr: 0.01999301244215147\n",
      "epoch: 800, acc: 0.877, loss: 0.451 (data_loss: 0.344, reg_loss: 0.108), lr: 0.0199920131907303\n",
      "epoch: 900, acc: 0.877, loss: 0.440 (data_loss: 0.333, reg_loss: 0.107), lr: 0.019991014039189386\n",
      "epoch: 1000, acc: 0.883, loss: 0.425 (data_loss: 0.319, reg_loss: 0.106), lr: 0.019990014987513734\n",
      "epoch: 1100, acc: 0.890, loss: 0.414 (data_loss: 0.309, reg_loss: 0.104), lr: 0.01998901603568839\n",
      "epoch: 1200, acc: 0.893, loss: 0.405 (data_loss: 0.302, reg_loss: 0.103), lr: 0.019988017183698373\n",
      "epoch: 1300, acc: 0.900, loss: 0.396 (data_loss: 0.294, reg_loss: 0.101), lr: 0.01998701843152872\n",
      "epoch: 1400, acc: 0.900, loss: 0.388 (data_loss: 0.288, reg_loss: 0.100), lr: 0.019986019779164473\n",
      "epoch: 1500, acc: 0.907, loss: 0.382 (data_loss: 0.283, reg_loss: 0.099), lr: 0.019985021226590672\n",
      "epoch: 1600, acc: 0.900, loss: 0.374 (data_loss: 0.277, reg_loss: 0.097), lr: 0.01998402277379235\n",
      "epoch: 1700, acc: 0.897, loss: 0.371 (data_loss: 0.275, reg_loss: 0.096), lr: 0.01998302442075457\n",
      "epoch: 1800, acc: 0.910, loss: 0.364 (data_loss: 0.269, reg_loss: 0.095), lr: 0.019982026167462367\n",
      "epoch: 1900, acc: 0.913, loss: 0.358 (data_loss: 0.264, reg_loss: 0.094), lr: 0.019981028013900805\n",
      "epoch: 2000, acc: 0.913, loss: 0.353 (data_loss: 0.260, reg_loss: 0.092), lr: 0.019980029960054924\n",
      "epoch: 2100, acc: 0.917, loss: 0.350 (data_loss: 0.259, reg_loss: 0.091), lr: 0.019979032005909798\n",
      "epoch: 2200, acc: 0.903, loss: 0.350 (data_loss: 0.260, reg_loss: 0.090), lr: 0.01997803415145048\n",
      "epoch: 2300, acc: 0.907, loss: 0.340 (data_loss: 0.251, reg_loss: 0.089), lr: 0.019977036396662037\n",
      "epoch: 2400, acc: 0.920, loss: 0.333 (data_loss: 0.245, reg_loss: 0.088), lr: 0.019976038741529537\n",
      "epoch: 2500, acc: 0.920, loss: 0.331 (data_loss: 0.244, reg_loss: 0.087), lr: 0.01997504118603805\n",
      "epoch: 2600, acc: 0.920, loss: 0.326 (data_loss: 0.240, reg_loss: 0.086), lr: 0.01997404373017264\n",
      "epoch: 2700, acc: 0.920, loss: 0.323 (data_loss: 0.238, reg_loss: 0.085), lr: 0.0199730463739184\n",
      "epoch: 2800, acc: 0.910, loss: 0.325 (data_loss: 0.240, reg_loss: 0.085), lr: 0.019972049117260395\n",
      "epoch: 2900, acc: 0.927, loss: 0.316 (data_loss: 0.232, reg_loss: 0.084), lr: 0.019971051960183714\n",
      "epoch: 3000, acc: 0.917, loss: 0.313 (data_loss: 0.230, reg_loss: 0.083), lr: 0.019970054902673444\n",
      "epoch: 3100, acc: 0.917, loss: 0.313 (data_loss: 0.230, reg_loss: 0.082), lr: 0.019969057944714663\n",
      "epoch: 3200, acc: 0.923, loss: 0.307 (data_loss: 0.226, reg_loss: 0.081), lr: 0.019968061086292475\n",
      "epoch: 3300, acc: 0.930, loss: 0.306 (data_loss: 0.225, reg_loss: 0.081), lr: 0.019967064327391967\n",
      "epoch: 3400, acc: 0.927, loss: 0.302 (data_loss: 0.222, reg_loss: 0.080), lr: 0.019966067667998237\n",
      "epoch: 3500, acc: 0.927, loss: 0.299 (data_loss: 0.220, reg_loss: 0.079), lr: 0.019965071108096383\n",
      "epoch: 3600, acc: 0.920, loss: 0.297 (data_loss: 0.219, reg_loss: 0.079), lr: 0.01996407464767152\n",
      "epoch: 3700, acc: 0.927, loss: 0.301 (data_loss: 0.220, reg_loss: 0.081), lr: 0.019963078286708732\n",
      "epoch: 3800, acc: 0.927, loss: 0.294 (data_loss: 0.214, reg_loss: 0.081), lr: 0.019962082025193145\n",
      "epoch: 3900, acc: 0.927, loss: 0.292 (data_loss: 0.212, reg_loss: 0.080), lr: 0.019961085863109868\n",
      "epoch: 4000, acc: 0.927, loss: 0.291 (data_loss: 0.212, reg_loss: 0.079), lr: 0.019960089800444013\n",
      "epoch: 4100, acc: 0.927, loss: 0.290 (data_loss: 0.211, reg_loss: 0.079), lr: 0.019959093837180697\n",
      "epoch: 4200, acc: 0.927, loss: 0.288 (data_loss: 0.210, reg_loss: 0.078), lr: 0.01995809797330505\n",
      "epoch: 4300, acc: 0.927, loss: 0.287 (data_loss: 0.209, reg_loss: 0.077), lr: 0.01995710220880218\n",
      "epoch: 4400, acc: 0.927, loss: 0.285 (data_loss: 0.209, reg_loss: 0.077), lr: 0.019956106543657228\n",
      "epoch: 4500, acc: 0.927, loss: 0.284 (data_loss: 0.208, reg_loss: 0.076), lr: 0.019955110977855316\n",
      "epoch: 4600, acc: 0.927, loss: 0.282 (data_loss: 0.207, reg_loss: 0.075), lr: 0.01995411551138158\n",
      "epoch: 4700, acc: 0.930, loss: 0.282 (data_loss: 0.207, reg_loss: 0.075), lr: 0.019953120144221154\n",
      "epoch: 4800, acc: 0.927, loss: 0.280 (data_loss: 0.205, reg_loss: 0.074), lr: 0.019952124876359174\n",
      "epoch: 4900, acc: 0.930, loss: 0.283 (data_loss: 0.209, reg_loss: 0.074), lr: 0.01995112970778079\n",
      "epoch: 5000, acc: 0.927, loss: 0.277 (data_loss: 0.204, reg_loss: 0.073), lr: 0.019950134638471142\n",
      "epoch: 5100, acc: 0.927, loss: 0.277 (data_loss: 0.205, reg_loss: 0.073), lr: 0.019949139668415376\n",
      "epoch: 5200, acc: 0.913, loss: 0.286 (data_loss: 0.214, reg_loss: 0.072), lr: 0.01994814479759864\n",
      "epoch: 5300, acc: 0.930, loss: 0.278 (data_loss: 0.206, reg_loss: 0.072), lr: 0.019947150026006097\n",
      "epoch: 5400, acc: 0.927, loss: 0.275 (data_loss: 0.204, reg_loss: 0.071), lr: 0.019946155353622895\n",
      "epoch: 5500, acc: 0.930, loss: 0.271 (data_loss: 0.200, reg_loss: 0.071), lr: 0.019945160780434196\n",
      "epoch: 5600, acc: 0.930, loss: 0.269 (data_loss: 0.198, reg_loss: 0.071), lr: 0.019944166306425162\n",
      "epoch: 5700, acc: 0.927, loss: 0.267 (data_loss: 0.197, reg_loss: 0.070), lr: 0.01994317193158096\n",
      "epoch: 5800, acc: 0.927, loss: 0.267 (data_loss: 0.197, reg_loss: 0.070), lr: 0.019942177655886757\n",
      "epoch: 5900, acc: 0.927, loss: 0.265 (data_loss: 0.195, reg_loss: 0.069), lr: 0.019941183479327725\n",
      "epoch: 6000, acc: 0.927, loss: 0.264 (data_loss: 0.195, reg_loss: 0.069), lr: 0.019940189401889033\n",
      "epoch: 6100, acc: 0.930, loss: 0.262 (data_loss: 0.194, reg_loss: 0.068), lr: 0.01993919542355587\n",
      "epoch: 6200, acc: 0.930, loss: 0.262 (data_loss: 0.194, reg_loss: 0.068), lr: 0.019938201544313403\n",
      "epoch: 6300, acc: 0.933, loss: 0.261 (data_loss: 0.194, reg_loss: 0.068), lr: 0.01993720776414682\n",
      "epoch: 6400, acc: 0.930, loss: 0.260 (data_loss: 0.192, reg_loss: 0.067), lr: 0.019936214083041307\n",
      "epoch: 6500, acc: 0.930, loss: 0.259 (data_loss: 0.192, reg_loss: 0.067), lr: 0.01993522050098206\n",
      "epoch: 6600, acc: 0.930, loss: 0.257 (data_loss: 0.190, reg_loss: 0.067), lr: 0.019934227017954262\n",
      "epoch: 6700, acc: 0.927, loss: 0.257 (data_loss: 0.190, reg_loss: 0.066), lr: 0.01993323363394311\n",
      "epoch: 6800, acc: 0.933, loss: 0.267 (data_loss: 0.201, reg_loss: 0.066), lr: 0.0199322403489338\n",
      "epoch: 6900, acc: 0.933, loss: 0.253 (data_loss: 0.187, reg_loss: 0.066), lr: 0.019931247162911534\n",
      "epoch: 7000, acc: 0.937, loss: 0.252 (data_loss: 0.187, reg_loss: 0.065), lr: 0.019930254075861523\n",
      "epoch: 7100, acc: 0.930, loss: 0.251 (data_loss: 0.186, reg_loss: 0.065), lr: 0.019929261087768962\n",
      "epoch: 7200, acc: 0.933, loss: 0.249 (data_loss: 0.184, reg_loss: 0.065), lr: 0.01992826819861907\n",
      "epoch: 7300, acc: 0.937, loss: 0.248 (data_loss: 0.184, reg_loss: 0.064), lr: 0.019927275408397054\n",
      "epoch: 7400, acc: 0.943, loss: 0.256 (data_loss: 0.193, reg_loss: 0.064), lr: 0.019926282717088132\n",
      "epoch: 7500, acc: 0.937, loss: 0.253 (data_loss: 0.189, reg_loss: 0.064), lr: 0.01992529012467752\n",
      "epoch: 7600, acc: 0.923, loss: 0.252 (data_loss: 0.188, reg_loss: 0.063), lr: 0.019924297631150445\n",
      "epoch: 7700, acc: 0.930, loss: 0.254 (data_loss: 0.191, reg_loss: 0.063), lr: 0.019923305236492123\n",
      "epoch: 7800, acc: 0.940, loss: 0.250 (data_loss: 0.187, reg_loss: 0.063), lr: 0.01992231294068779\n",
      "epoch: 7900, acc: 0.930, loss: 0.242 (data_loss: 0.179, reg_loss: 0.062), lr: 0.019921320743722666\n",
      "epoch: 8000, acc: 0.917, loss: 0.269 (data_loss: 0.203, reg_loss: 0.067), lr: 0.019920328645582\n",
      "epoch: 8100, acc: 0.930, loss: 0.249 (data_loss: 0.183, reg_loss: 0.066), lr: 0.019919336646251007\n",
      "epoch: 8200, acc: 0.930, loss: 0.248 (data_loss: 0.182, reg_loss: 0.065), lr: 0.019918344745714942\n",
      "epoch: 8300, acc: 0.930, loss: 0.247 (data_loss: 0.182, reg_loss: 0.065), lr: 0.019917352943959042\n",
      "epoch: 8400, acc: 0.930, loss: 0.246 (data_loss: 0.182, reg_loss: 0.065), lr: 0.019916361240968555\n",
      "epoch: 8500, acc: 0.930, loss: 0.245 (data_loss: 0.181, reg_loss: 0.064), lr: 0.01991536963672872\n",
      "epoch: 8600, acc: 0.930, loss: 0.245 (data_loss: 0.181, reg_loss: 0.064), lr: 0.019914378131224802\n",
      "epoch: 8700, acc: 0.930, loss: 0.244 (data_loss: 0.181, reg_loss: 0.063), lr: 0.01991338672444204\n",
      "epoch: 8800, acc: 0.930, loss: 0.243 (data_loss: 0.180, reg_loss: 0.063), lr: 0.0199123954163657\n",
      "epoch: 8900, acc: 0.930, loss: 0.242 (data_loss: 0.179, reg_loss: 0.063), lr: 0.019911404206981037\n",
      "epoch: 9000, acc: 0.937, loss: 0.237 (data_loss: 0.175, reg_loss: 0.062), lr: 0.019910413096273318\n",
      "epoch: 9100, acc: 0.933, loss: 0.237 (data_loss: 0.175, reg_loss: 0.062), lr: 0.019909422084227805\n",
      "epoch: 9200, acc: 0.937, loss: 0.235 (data_loss: 0.173, reg_loss: 0.062), lr: 0.019908431170829768\n",
      "epoch: 9300, acc: 0.937, loss: 0.233 (data_loss: 0.172, reg_loss: 0.062), lr: 0.01990744035606448\n",
      "epoch: 9400, acc: 0.943, loss: 0.235 (data_loss: 0.174, reg_loss: 0.061), lr: 0.01990644963991721\n",
      "epoch: 9500, acc: 0.937, loss: 0.232 (data_loss: 0.171, reg_loss: 0.061), lr: 0.01990545902237324\n",
      "epoch: 9600, acc: 0.943, loss: 0.232 (data_loss: 0.171, reg_loss: 0.061), lr: 0.019904468503417844\n",
      "epoch: 9700, acc: 0.933, loss: 0.232 (data_loss: 0.171, reg_loss: 0.061), lr: 0.019903478083036316\n",
      "epoch: 9800, acc: 0.937, loss: 0.230 (data_loss: 0.169, reg_loss: 0.061), lr: 0.019902487761213932\n",
      "epoch: 9900, acc: 0.937, loss: 0.229 (data_loss: 0.169, reg_loss: 0.060), lr: 0.019901497537935988\n",
      "epoch: 10000, acc: 0.947, loss: 0.232 (data_loss: 0.172, reg_loss: 0.060), lr: 0.019900507413187767\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64, weight_regularizer_l2=5e-4, bias_regularizer_l2=5e-4)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_Adam(learning_rate=0.02, decay=5e-7)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    data_loss = loss_activation.forward(dense2.output, y)\n",
    "    # Calculate regularization penalty\n",
    "    regularization_loss = loss_activation.loss.regularization_loss(dense1) + loss_activation.loss.regularization_loss(dense2)\n",
    "    # Calculate overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "        f'acc: {accuracy:.3f}, ' +\n",
    "        f'loss: {loss:.3f} (' +\n",
    "        f'data_loss: {data_loss:.3f}, ' +\n",
    "        f'reg_loss: {regularization_loss:.3f}), ' +\n",
    "        f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f93b818d-5b7f-467a-85f3-2ef0569b8893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation, acc: 0.840, loss: 0.462\n"
     ]
    }
   ],
   "source": [
    "# Validate the model\n",
    "# Create test dataset\n",
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "# Perform a forward pass of our testing data through this layer\n",
    "dense1.forward(X_test)\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "# Perform a forward pass through the activation/loss function\n",
    "# takes the output of second dense layer here and returns loss\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predictions==y_test)\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2757fdd6-e2aa-421d-980d-8b018a5820b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.391, loss: 1.099 (data_loss: 1.099, reg_loss: 0.000), lr: 0.02\n",
      "epoch: 100, acc: 0.642, loss: 0.922 (data_loss: 0.894, reg_loss: 0.029), lr: 0.019999010049002574\n",
      "epoch: 200, acc: 0.744, loss: 0.749 (data_loss: 0.680, reg_loss: 0.069), lr: 0.019998010197985302\n",
      "epoch: 300, acc: 0.794, loss: 0.653 (data_loss: 0.561, reg_loss: 0.092), lr: 0.019997010446938183\n",
      "epoch: 400, acc: 0.812, loss: 0.602 (data_loss: 0.501, reg_loss: 0.100), lr: 0.01999601079584623\n",
      "epoch: 500, acc: 0.826, loss: 0.569 (data_loss: 0.466, reg_loss: 0.102), lr: 0.01999501124469445\n",
      "epoch: 600, acc: 0.840, loss: 0.541 (data_loss: 0.438, reg_loss: 0.103), lr: 0.01999401179346786\n",
      "epoch: 700, acc: 0.843, loss: 0.519 (data_loss: 0.417, reg_loss: 0.102), lr: 0.01999301244215147\n",
      "epoch: 800, acc: 0.853, loss: 0.501 (data_loss: 0.401, reg_loss: 0.100), lr: 0.0199920131907303\n",
      "epoch: 900, acc: 0.856, loss: 0.487 (data_loss: 0.389, reg_loss: 0.098), lr: 0.019991014039189386\n",
      "epoch: 1000, acc: 0.855, loss: 0.476 (data_loss: 0.380, reg_loss: 0.096), lr: 0.019990014987513734\n",
      "epoch: 1100, acc: 0.861, loss: 0.465 (data_loss: 0.371, reg_loss: 0.094), lr: 0.01998901603568839\n",
      "epoch: 1200, acc: 0.860, loss: 0.456 (data_loss: 0.365, reg_loss: 0.092), lr: 0.019988017183698373\n",
      "epoch: 1300, acc: 0.863, loss: 0.448 (data_loss: 0.358, reg_loss: 0.090), lr: 0.01998701843152872\n",
      "epoch: 1400, acc: 0.875, loss: 0.441 (data_loss: 0.352, reg_loss: 0.089), lr: 0.019986019779164473\n",
      "epoch: 1500, acc: 0.871, loss: 0.434 (data_loss: 0.347, reg_loss: 0.087), lr: 0.019985021226590672\n",
      "epoch: 1600, acc: 0.874, loss: 0.426 (data_loss: 0.341, reg_loss: 0.085), lr: 0.01998402277379235\n",
      "epoch: 1700, acc: 0.873, loss: 0.422 (data_loss: 0.338, reg_loss: 0.084), lr: 0.01998302442075457\n",
      "epoch: 1800, acc: 0.873, loss: 0.417 (data_loss: 0.335, reg_loss: 0.082), lr: 0.019982026167462367\n",
      "epoch: 1900, acc: 0.880, loss: 0.411 (data_loss: 0.330, reg_loss: 0.081), lr: 0.019981028013900805\n",
      "epoch: 2000, acc: 0.881, loss: 0.407 (data_loss: 0.327, reg_loss: 0.080), lr: 0.019980029960054924\n",
      "epoch: 2100, acc: 0.883, loss: 0.403 (data_loss: 0.325, reg_loss: 0.078), lr: 0.019979032005909798\n",
      "epoch: 2200, acc: 0.883, loss: 0.400 (data_loss: 0.323, reg_loss: 0.077), lr: 0.01997803415145048\n",
      "epoch: 2300, acc: 0.883, loss: 0.398 (data_loss: 0.322, reg_loss: 0.076), lr: 0.019977036396662037\n",
      "epoch: 2400, acc: 0.882, loss: 0.394 (data_loss: 0.319, reg_loss: 0.075), lr: 0.019976038741529537\n",
      "epoch: 2500, acc: 0.886, loss: 0.390 (data_loss: 0.316, reg_loss: 0.074), lr: 0.01997504118603805\n",
      "epoch: 2600, acc: 0.885, loss: 0.387 (data_loss: 0.314, reg_loss: 0.073), lr: 0.01997404373017264\n",
      "epoch: 2700, acc: 0.875, loss: 0.388 (data_loss: 0.316, reg_loss: 0.072), lr: 0.0199730463739184\n",
      "epoch: 2800, acc: 0.885, loss: 0.380 (data_loss: 0.309, reg_loss: 0.071), lr: 0.019972049117260395\n",
      "epoch: 2900, acc: 0.886, loss: 0.378 (data_loss: 0.308, reg_loss: 0.070), lr: 0.019971051960183714\n",
      "epoch: 3000, acc: 0.885, loss: 0.376 (data_loss: 0.306, reg_loss: 0.070), lr: 0.019970054902673444\n",
      "epoch: 3100, acc: 0.884, loss: 0.373 (data_loss: 0.304, reg_loss: 0.069), lr: 0.019969057944714663\n",
      "epoch: 3200, acc: 0.887, loss: 0.371 (data_loss: 0.303, reg_loss: 0.068), lr: 0.019968061086292475\n",
      "epoch: 3300, acc: 0.889, loss: 0.369 (data_loss: 0.301, reg_loss: 0.068), lr: 0.019967064327391967\n",
      "epoch: 3400, acc: 0.886, loss: 0.366 (data_loss: 0.300, reg_loss: 0.067), lr: 0.019966067667998237\n",
      "epoch: 3500, acc: 0.887, loss: 0.364 (data_loss: 0.298, reg_loss: 0.066), lr: 0.019965071108096383\n",
      "epoch: 3600, acc: 0.892, loss: 0.363 (data_loss: 0.296, reg_loss: 0.067), lr: 0.01996407464767152\n",
      "epoch: 3700, acc: 0.892, loss: 0.359 (data_loss: 0.292, reg_loss: 0.067), lr: 0.019963078286708732\n",
      "epoch: 3800, acc: 0.893, loss: 0.357 (data_loss: 0.291, reg_loss: 0.066), lr: 0.019962082025193145\n",
      "epoch: 3900, acc: 0.893, loss: 0.356 (data_loss: 0.290, reg_loss: 0.065), lr: 0.019961085863109868\n",
      "epoch: 4000, acc: 0.893, loss: 0.354 (data_loss: 0.289, reg_loss: 0.065), lr: 0.019960089800444013\n",
      "epoch: 4100, acc: 0.892, loss: 0.353 (data_loss: 0.289, reg_loss: 0.064), lr: 0.019959093837180697\n",
      "epoch: 4200, acc: 0.894, loss: 0.352 (data_loss: 0.288, reg_loss: 0.064), lr: 0.01995809797330505\n",
      "epoch: 4300, acc: 0.891, loss: 0.352 (data_loss: 0.289, reg_loss: 0.063), lr: 0.01995710220880218\n",
      "epoch: 4400, acc: 0.891, loss: 0.351 (data_loss: 0.288, reg_loss: 0.062), lr: 0.019956106543657228\n",
      "epoch: 4500, acc: 0.891, loss: 0.350 (data_loss: 0.288, reg_loss: 0.062), lr: 0.019955110977855316\n",
      "epoch: 4600, acc: 0.883, loss: 0.352 (data_loss: 0.290, reg_loss: 0.061), lr: 0.01995411551138158\n",
      "epoch: 4700, acc: 0.892, loss: 0.346 (data_loss: 0.285, reg_loss: 0.061), lr: 0.019953120144221154\n",
      "epoch: 4800, acc: 0.893, loss: 0.345 (data_loss: 0.285, reg_loss: 0.060), lr: 0.019952124876359174\n",
      "epoch: 4900, acc: 0.896, loss: 0.343 (data_loss: 0.283, reg_loss: 0.060), lr: 0.01995112970778079\n",
      "epoch: 5000, acc: 0.893, loss: 0.342 (data_loss: 0.282, reg_loss: 0.059), lr: 0.019950134638471142\n",
      "epoch: 5100, acc: 0.890, loss: 0.343 (data_loss: 0.284, reg_loss: 0.059), lr: 0.019949139668415376\n",
      "epoch: 5200, acc: 0.895, loss: 0.338 (data_loss: 0.279, reg_loss: 0.058), lr: 0.01994814479759864\n",
      "epoch: 5300, acc: 0.896, loss: 0.338 (data_loss: 0.280, reg_loss: 0.058), lr: 0.019947150026006097\n",
      "epoch: 5400, acc: 0.897, loss: 0.336 (data_loss: 0.279, reg_loss: 0.058), lr: 0.019946155353622895\n",
      "epoch: 5500, acc: 0.895, loss: 0.334 (data_loss: 0.277, reg_loss: 0.057), lr: 0.019945160780434196\n",
      "epoch: 5600, acc: 0.888, loss: 0.338 (data_loss: 0.282, reg_loss: 0.057), lr: 0.019944166306425162\n",
      "epoch: 5700, acc: 0.896, loss: 0.332 (data_loss: 0.276, reg_loss: 0.056), lr: 0.01994317193158096\n",
      "epoch: 5800, acc: 0.895, loss: 0.333 (data_loss: 0.277, reg_loss: 0.056), lr: 0.019942177655886757\n",
      "epoch: 5900, acc: 0.895, loss: 0.331 (data_loss: 0.276, reg_loss: 0.056), lr: 0.019941183479327725\n",
      "epoch: 6000, acc: 0.897, loss: 0.329 (data_loss: 0.274, reg_loss: 0.055), lr: 0.019940189401889033\n",
      "epoch: 6100, acc: 0.894, loss: 0.331 (data_loss: 0.276, reg_loss: 0.055), lr: 0.01993919542355587\n",
      "epoch: 6200, acc: 0.896, loss: 0.329 (data_loss: 0.275, reg_loss: 0.054), lr: 0.019938201544313403\n",
      "epoch: 6300, acc: 0.896, loss: 0.327 (data_loss: 0.273, reg_loss: 0.054), lr: 0.01993720776414682\n",
      "epoch: 6400, acc: 0.898, loss: 0.325 (data_loss: 0.272, reg_loss: 0.054), lr: 0.019936214083041307\n",
      "epoch: 6500, acc: 0.896, loss: 0.325 (data_loss: 0.271, reg_loss: 0.053), lr: 0.01993522050098206\n",
      "epoch: 6600, acc: 0.897, loss: 0.328 (data_loss: 0.272, reg_loss: 0.055), lr: 0.019934227017954262\n",
      "epoch: 6700, acc: 0.896, loss: 0.323 (data_loss: 0.268, reg_loss: 0.055), lr: 0.01993323363394311\n",
      "epoch: 6800, acc: 0.896, loss: 0.322 (data_loss: 0.268, reg_loss: 0.055), lr: 0.0199322403489338\n",
      "epoch: 6900, acc: 0.896, loss: 0.322 (data_loss: 0.267, reg_loss: 0.054), lr: 0.019931247162911534\n",
      "epoch: 7000, acc: 0.896, loss: 0.321 (data_loss: 0.267, reg_loss: 0.054), lr: 0.019930254075861523\n",
      "epoch: 7100, acc: 0.897, loss: 0.320 (data_loss: 0.267, reg_loss: 0.054), lr: 0.019929261087768962\n",
      "epoch: 7200, acc: 0.897, loss: 0.320 (data_loss: 0.266, reg_loss: 0.053), lr: 0.01992826819861907\n",
      "epoch: 7300, acc: 0.898, loss: 0.319 (data_loss: 0.266, reg_loss: 0.053), lr: 0.019927275408397054\n",
      "epoch: 7400, acc: 0.898, loss: 0.318 (data_loss: 0.266, reg_loss: 0.053), lr: 0.019926282717088132\n",
      "epoch: 7500, acc: 0.889, loss: 0.327 (data_loss: 0.274, reg_loss: 0.052), lr: 0.01992529012467752\n",
      "epoch: 7600, acc: 0.896, loss: 0.318 (data_loss: 0.266, reg_loss: 0.052), lr: 0.019924297631150445\n",
      "epoch: 7700, acc: 0.893, loss: 0.321 (data_loss: 0.269, reg_loss: 0.052), lr: 0.019923305236492123\n",
      "epoch: 7800, acc: 0.899, loss: 0.315 (data_loss: 0.264, reg_loss: 0.052), lr: 0.01992231294068779\n",
      "epoch: 7900, acc: 0.898, loss: 0.315 (data_loss: 0.264, reg_loss: 0.051), lr: 0.019921320743722666\n",
      "epoch: 8000, acc: 0.898, loss: 0.316 (data_loss: 0.264, reg_loss: 0.051), lr: 0.019920328645582\n",
      "epoch: 8100, acc: 0.899, loss: 0.314 (data_loss: 0.263, reg_loss: 0.051), lr: 0.019919336646251007\n",
      "epoch: 8200, acc: 0.898, loss: 0.313 (data_loss: 0.262, reg_loss: 0.051), lr: 0.019918344745714942\n",
      "epoch: 8300, acc: 0.899, loss: 0.314 (data_loss: 0.264, reg_loss: 0.050), lr: 0.019917352943959042\n",
      "epoch: 8400, acc: 0.897, loss: 0.315 (data_loss: 0.265, reg_loss: 0.050), lr: 0.019916361240968555\n",
      "epoch: 8500, acc: 0.899, loss: 0.312 (data_loss: 0.262, reg_loss: 0.050), lr: 0.01991536963672872\n",
      "epoch: 8600, acc: 0.901, loss: 0.312 (data_loss: 0.262, reg_loss: 0.050), lr: 0.019914378131224802\n",
      "epoch: 8700, acc: 0.898, loss: 0.310 (data_loss: 0.261, reg_loss: 0.049), lr: 0.01991338672444204\n",
      "epoch: 8800, acc: 0.901, loss: 0.310 (data_loss: 0.261, reg_loss: 0.049), lr: 0.0199123954163657\n",
      "epoch: 8900, acc: 0.901, loss: 0.313 (data_loss: 0.264, reg_loss: 0.049), lr: 0.019911404206981037\n",
      "epoch: 9000, acc: 0.903, loss: 0.309 (data_loss: 0.260, reg_loss: 0.049), lr: 0.019910413096273318\n",
      "epoch: 9100, acc: 0.900, loss: 0.308 (data_loss: 0.259, reg_loss: 0.048), lr: 0.019909422084227805\n",
      "epoch: 9200, acc: 0.899, loss: 0.307 (data_loss: 0.259, reg_loss: 0.048), lr: 0.019908431170829768\n",
      "epoch: 9300, acc: 0.898, loss: 0.306 (data_loss: 0.258, reg_loss: 0.048), lr: 0.01990744035606448\n",
      "epoch: 9400, acc: 0.901, loss: 0.308 (data_loss: 0.260, reg_loss: 0.048), lr: 0.01990644963991721\n",
      "epoch: 9500, acc: 0.899, loss: 0.308 (data_loss: 0.260, reg_loss: 0.048), lr: 0.01990545902237324\n",
      "epoch: 9600, acc: 0.897, loss: 0.307 (data_loss: 0.259, reg_loss: 0.047), lr: 0.019904468503417844\n",
      "epoch: 9700, acc: 0.895, loss: 0.311 (data_loss: 0.264, reg_loss: 0.047), lr: 0.019903478083036316\n",
      "epoch: 9800, acc: 0.900, loss: 0.303 (data_loss: 0.257, reg_loss: 0.047), lr: 0.019902487761213932\n",
      "epoch: 9900, acc: 0.900, loss: 0.302 (data_loss: 0.256, reg_loss: 0.047), lr: 0.019901497537935988\n",
      "epoch: 10000, acc: 0.901, loss: 0.302 (data_loss: 0.255, reg_loss: 0.046), lr: 0.019900507413187767\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data(samples=1000, classes=3)\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64, weight_regularizer_l2=5e-4, bias_regularizer_l2=5e-4)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_Adam(learning_rate=0.02, decay=5e-7)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    data_loss = loss_activation.forward(dense2.output, y)\n",
    "    # Calculate regularization penalty\n",
    "    regularization_loss = loss_activation.loss.regularization_loss(dense1) + loss_activation.loss.regularization_loss(dense2)\n",
    "    # Calculate overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "        f'acc: {accuracy:.3f}, ' +\n",
    "        f'loss: {loss:.3f} (' +\n",
    "        f'data_loss: {data_loss:.3f}, ' +\n",
    "        f'reg_loss: {regularization_loss:.3f}), ' +\n",
    "        f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ce7ffcd-f389-49bf-8388-b1584190db67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation, acc: 0.907, loss: 0.275\n"
     ]
    }
   ],
   "source": [
    "# Validate the model\n",
    "# Create test dataset\n",
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "# Perform a forward pass of our testing data through this layer\n",
    "dense1.forward(X_test)\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "# Perform a forward pass through the activation/loss function\n",
    "# takes the output of second dense layer here and returns loss\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predictions==y_test)\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12c43401-42bc-4f97-a85e-2dd62a89b031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.302, loss: 1.099 (data_loss: 1.099, reg_loss: 0.000), lr: 0.02\n",
      "epoch: 100, acc: 0.699, loss: 0.793 (data_loss: 0.742, reg_loss: 0.051), lr: 0.019999010049002574\n",
      "epoch: 200, acc: 0.809, loss: 0.624 (data_loss: 0.537, reg_loss: 0.087), lr: 0.019998010197985302\n",
      "epoch: 300, acc: 0.841, loss: 0.548 (data_loss: 0.449, reg_loss: 0.098), lr: 0.019997010446938183\n",
      "epoch: 400, acc: 0.867, loss: 0.501 (data_loss: 0.400, reg_loss: 0.101), lr: 0.01999601079584623\n",
      "epoch: 500, acc: 0.872, loss: 0.469 (data_loss: 0.369, reg_loss: 0.099), lr: 0.01999501124469445\n",
      "epoch: 600, acc: 0.881, loss: 0.446 (data_loss: 0.350, reg_loss: 0.097), lr: 0.01999401179346786\n",
      "epoch: 700, acc: 0.885, loss: 0.428 (data_loss: 0.334, reg_loss: 0.094), lr: 0.01999301244215147\n",
      "epoch: 800, acc: 0.887, loss: 0.416 (data_loss: 0.325, reg_loss: 0.091), lr: 0.0199920131907303\n",
      "epoch: 900, acc: 0.889, loss: 0.406 (data_loss: 0.317, reg_loss: 0.088), lr: 0.019991014039189386\n",
      "epoch: 1000, acc: 0.894, loss: 0.396 (data_loss: 0.310, reg_loss: 0.086), lr: 0.019990014987513734\n",
      "epoch: 1100, acc: 0.892, loss: 0.384 (data_loss: 0.301, reg_loss: 0.083), lr: 0.01998901603568839\n",
      "epoch: 1200, acc: 0.896, loss: 0.379 (data_loss: 0.298, reg_loss: 0.081), lr: 0.019988017183698373\n",
      "epoch: 1300, acc: 0.897, loss: 0.370 (data_loss: 0.291, reg_loss: 0.079), lr: 0.01998701843152872\n",
      "epoch: 1400, acc: 0.896, loss: 0.366 (data_loss: 0.289, reg_loss: 0.077), lr: 0.019986019779164473\n",
      "epoch: 1500, acc: 0.893, loss: 0.360 (data_loss: 0.285, reg_loss: 0.075), lr: 0.019985021226590672\n",
      "epoch: 1600, acc: 0.896, loss: 0.355 (data_loss: 0.281, reg_loss: 0.074), lr: 0.01998402277379235\n",
      "epoch: 1700, acc: 0.899, loss: 0.352 (data_loss: 0.280, reg_loss: 0.072), lr: 0.01998302442075457\n",
      "epoch: 1800, acc: 0.902, loss: 0.347 (data_loss: 0.276, reg_loss: 0.071), lr: 0.019982026167462367\n",
      "epoch: 1900, acc: 0.901, loss: 0.343 (data_loss: 0.273, reg_loss: 0.070), lr: 0.019981028013900805\n",
      "epoch: 2000, acc: 0.900, loss: 0.339 (data_loss: 0.270, reg_loss: 0.069), lr: 0.019980029960054924\n",
      "epoch: 2100, acc: 0.901, loss: 0.339 (data_loss: 0.271, reg_loss: 0.068), lr: 0.019979032005909798\n",
      "epoch: 2200, acc: 0.902, loss: 0.335 (data_loss: 0.269, reg_loss: 0.066), lr: 0.01997803415145048\n",
      "epoch: 2300, acc: 0.904, loss: 0.331 (data_loss: 0.266, reg_loss: 0.065), lr: 0.019977036396662037\n",
      "epoch: 2400, acc: 0.897, loss: 0.330 (data_loss: 0.266, reg_loss: 0.064), lr: 0.019976038741529537\n",
      "epoch: 2500, acc: 0.905, loss: 0.326 (data_loss: 0.262, reg_loss: 0.063), lr: 0.01997504118603805\n",
      "epoch: 2600, acc: 0.644, loss: 1.415 (data_loss: 1.347, reg_loss: 0.068), lr: 0.01997404373017264\n",
      "epoch: 2700, acc: 0.903, loss: 0.333 (data_loss: 0.259, reg_loss: 0.074), lr: 0.0199730463739184\n",
      "epoch: 2800, acc: 0.902, loss: 0.328 (data_loss: 0.257, reg_loss: 0.071), lr: 0.019972049117260395\n",
      "epoch: 2900, acc: 0.902, loss: 0.326 (data_loss: 0.257, reg_loss: 0.069), lr: 0.019971051960183714\n",
      "epoch: 3000, acc: 0.902, loss: 0.323 (data_loss: 0.256, reg_loss: 0.067), lr: 0.019970054902673444\n",
      "epoch: 3100, acc: 0.903, loss: 0.321 (data_loss: 0.255, reg_loss: 0.066), lr: 0.019969057944714663\n",
      "epoch: 3200, acc: 0.903, loss: 0.320 (data_loss: 0.254, reg_loss: 0.065), lr: 0.019968061086292475\n",
      "epoch: 3300, acc: 0.903, loss: 0.318 (data_loss: 0.254, reg_loss: 0.064), lr: 0.019967064327391967\n",
      "epoch: 3400, acc: 0.903, loss: 0.316 (data_loss: 0.253, reg_loss: 0.063), lr: 0.019966067667998237\n",
      "epoch: 3500, acc: 0.903, loss: 0.315 (data_loss: 0.252, reg_loss: 0.062), lr: 0.019965071108096383\n",
      "epoch: 3600, acc: 0.905, loss: 0.313 (data_loss: 0.252, reg_loss: 0.061), lr: 0.01996407464767152\n",
      "epoch: 3700, acc: 0.905, loss: 0.311 (data_loss: 0.251, reg_loss: 0.061), lr: 0.019963078286708732\n",
      "epoch: 3800, acc: 0.906, loss: 0.311 (data_loss: 0.251, reg_loss: 0.060), lr: 0.019962082025193145\n",
      "epoch: 3900, acc: 0.906, loss: 0.308 (data_loss: 0.249, reg_loss: 0.059), lr: 0.019961085863109868\n",
      "epoch: 4000, acc: 0.902, loss: 0.308 (data_loss: 0.249, reg_loss: 0.058), lr: 0.019960089800444013\n",
      "epoch: 4100, acc: 0.903, loss: 0.307 (data_loss: 0.249, reg_loss: 0.058), lr: 0.019959093837180697\n",
      "epoch: 4200, acc: 0.905, loss: 0.310 (data_loss: 0.253, reg_loss: 0.057), lr: 0.01995809797330505\n",
      "epoch: 4300, acc: 0.908, loss: 0.305 (data_loss: 0.248, reg_loss: 0.057), lr: 0.01995710220880218\n",
      "epoch: 4400, acc: 0.902, loss: 0.304 (data_loss: 0.248, reg_loss: 0.056), lr: 0.019956106543657228\n",
      "epoch: 4500, acc: 0.909, loss: 0.300 (data_loss: 0.244, reg_loss: 0.055), lr: 0.019955110977855316\n",
      "epoch: 4600, acc: 0.908, loss: 0.304 (data_loss: 0.249, reg_loss: 0.055), lr: 0.01995411551138158\n",
      "epoch: 4700, acc: 0.908, loss: 0.299 (data_loss: 0.244, reg_loss: 0.054), lr: 0.019953120144221154\n",
      "epoch: 4800, acc: 0.905, loss: 0.296 (data_loss: 0.242, reg_loss: 0.054), lr: 0.019952124876359174\n",
      "epoch: 4900, acc: 0.899, loss: 0.301 (data_loss: 0.248, reg_loss: 0.053), lr: 0.01995112970778079\n",
      "epoch: 5000, acc: 0.905, loss: 0.295 (data_loss: 0.242, reg_loss: 0.053), lr: 0.019950134638471142\n",
      "epoch: 5100, acc: 0.910, loss: 0.294 (data_loss: 0.242, reg_loss: 0.052), lr: 0.019949139668415376\n",
      "epoch: 5200, acc: 0.908, loss: 0.291 (data_loss: 0.239, reg_loss: 0.052), lr: 0.01994814479759864\n",
      "epoch: 5300, acc: 0.901, loss: 0.296 (data_loss: 0.245, reg_loss: 0.051), lr: 0.019947150026006097\n",
      "epoch: 5400, acc: 0.908, loss: 0.296 (data_loss: 0.245, reg_loss: 0.051), lr: 0.019946155353622895\n",
      "epoch: 5500, acc: 0.910, loss: 0.288 (data_loss: 0.238, reg_loss: 0.050), lr: 0.019945160780434196\n",
      "epoch: 5600, acc: 0.909, loss: 0.291 (data_loss: 0.240, reg_loss: 0.050), lr: 0.019944166306425162\n",
      "epoch: 5700, acc: 0.904, loss: 0.291 (data_loss: 0.241, reg_loss: 0.050), lr: 0.01994317193158096\n",
      "epoch: 5800, acc: 0.905, loss: 0.288 (data_loss: 0.239, reg_loss: 0.049), lr: 0.019942177655886757\n",
      "epoch: 5900, acc: 0.904, loss: 0.287 (data_loss: 0.239, reg_loss: 0.049), lr: 0.019941183479327725\n",
      "epoch: 6000, acc: 0.913, loss: 0.284 (data_loss: 0.236, reg_loss: 0.048), lr: 0.019940189401889033\n",
      "epoch: 6100, acc: 0.911, loss: 0.282 (data_loss: 0.234, reg_loss: 0.048), lr: 0.01993919542355587\n",
      "epoch: 6200, acc: 0.910, loss: 0.281 (data_loss: 0.233, reg_loss: 0.048), lr: 0.019938201544313403\n",
      "epoch: 6300, acc: 0.903, loss: 0.285 (data_loss: 0.238, reg_loss: 0.047), lr: 0.01993720776414682\n",
      "epoch: 6400, acc: 0.912, loss: 0.280 (data_loss: 0.233, reg_loss: 0.047), lr: 0.019936214083041307\n",
      "epoch: 6500, acc: 0.912, loss: 0.278 (data_loss: 0.232, reg_loss: 0.046), lr: 0.01993522050098206\n",
      "epoch: 6600, acc: 0.783, loss: 0.665 (data_loss: 0.615, reg_loss: 0.050), lr: 0.019934227017954262\n",
      "epoch: 6700, acc: 0.908, loss: 0.288 (data_loss: 0.235, reg_loss: 0.053), lr: 0.01993323363394311\n",
      "epoch: 6800, acc: 0.907, loss: 0.285 (data_loss: 0.233, reg_loss: 0.052), lr: 0.0199322403489338\n",
      "epoch: 6900, acc: 0.909, loss: 0.283 (data_loss: 0.232, reg_loss: 0.051), lr: 0.019931247162911534\n",
      "epoch: 7000, acc: 0.909, loss: 0.282 (data_loss: 0.231, reg_loss: 0.050), lr: 0.019930254075861523\n",
      "epoch: 7100, acc: 0.911, loss: 0.281 (data_loss: 0.231, reg_loss: 0.050), lr: 0.019929261087768962\n",
      "epoch: 7200, acc: 0.909, loss: 0.280 (data_loss: 0.230, reg_loss: 0.049), lr: 0.01992826819861907\n",
      "epoch: 7300, acc: 0.911, loss: 0.279 (data_loss: 0.230, reg_loss: 0.049), lr: 0.019927275408397054\n",
      "epoch: 7400, acc: 0.911, loss: 0.278 (data_loss: 0.230, reg_loss: 0.048), lr: 0.019926282717088132\n",
      "epoch: 7500, acc: 0.911, loss: 0.277 (data_loss: 0.229, reg_loss: 0.048), lr: 0.01992529012467752\n",
      "epoch: 7600, acc: 0.911, loss: 0.276 (data_loss: 0.229, reg_loss: 0.047), lr: 0.019924297631150445\n",
      "epoch: 7700, acc: 0.913, loss: 0.278 (data_loss: 0.231, reg_loss: 0.047), lr: 0.019923305236492123\n",
      "epoch: 7800, acc: 0.912, loss: 0.275 (data_loss: 0.228, reg_loss: 0.046), lr: 0.01992231294068779\n",
      "epoch: 7900, acc: 0.906, loss: 0.280 (data_loss: 0.234, reg_loss: 0.046), lr: 0.019921320743722666\n",
      "epoch: 8000, acc: 0.905, loss: 0.278 (data_loss: 0.232, reg_loss: 0.046), lr: 0.019920328645582\n",
      "epoch: 8100, acc: 0.909, loss: 0.273 (data_loss: 0.227, reg_loss: 0.045), lr: 0.019919336646251007\n",
      "epoch: 8200, acc: 0.913, loss: 0.272 (data_loss: 0.227, reg_loss: 0.045), lr: 0.019918344745714942\n",
      "epoch: 8300, acc: 0.911, loss: 0.277 (data_loss: 0.233, reg_loss: 0.045), lr: 0.019917352943959042\n",
      "epoch: 8400, acc: 0.913, loss: 0.275 (data_loss: 0.230, reg_loss: 0.044), lr: 0.019916361240968555\n",
      "epoch: 8500, acc: 0.910, loss: 0.271 (data_loss: 0.227, reg_loss: 0.044), lr: 0.01991536963672872\n",
      "epoch: 8600, acc: 0.910, loss: 0.278 (data_loss: 0.235, reg_loss: 0.044), lr: 0.019914378131224802\n",
      "epoch: 8700, acc: 0.910, loss: 0.276 (data_loss: 0.232, reg_loss: 0.043), lr: 0.01991338672444204\n",
      "epoch: 8800, acc: 0.914, loss: 0.269 (data_loss: 0.226, reg_loss: 0.043), lr: 0.0199123954163657\n",
      "epoch: 8900, acc: 0.914, loss: 0.273 (data_loss: 0.230, reg_loss: 0.043), lr: 0.019911404206981037\n",
      "epoch: 9000, acc: 0.912, loss: 0.266 (data_loss: 0.224, reg_loss: 0.042), lr: 0.019910413096273318\n",
      "epoch: 9100, acc: 0.910, loss: 0.266 (data_loss: 0.224, reg_loss: 0.042), lr: 0.019909422084227805\n",
      "epoch: 9200, acc: 0.912, loss: 0.266 (data_loss: 0.224, reg_loss: 0.042), lr: 0.019908431170829768\n",
      "epoch: 9300, acc: 0.913, loss: 0.265 (data_loss: 0.223, reg_loss: 0.042), lr: 0.01990744035606448\n",
      "epoch: 9400, acc: 0.912, loss: 0.265 (data_loss: 0.224, reg_loss: 0.041), lr: 0.01990644963991721\n",
      "epoch: 9500, acc: 0.914, loss: 0.267 (data_loss: 0.226, reg_loss: 0.041), lr: 0.01990545902237324\n",
      "epoch: 9600, acc: 0.905, loss: 0.292 (data_loss: 0.251, reg_loss: 0.041), lr: 0.019904468503417844\n",
      "epoch: 9700, acc: 0.914, loss: 0.269 (data_loss: 0.228, reg_loss: 0.041), lr: 0.019903478083036316\n",
      "epoch: 9800, acc: 0.906, loss: 0.269 (data_loss: 0.229, reg_loss: 0.041), lr: 0.019902487761213932\n",
      "epoch: 9900, acc: 0.908, loss: 0.267 (data_loss: 0.227, reg_loss: 0.040), lr: 0.019901497537935988\n",
      "epoch: 10000, acc: 0.912, loss: 0.262 (data_loss: 0.222, reg_loss: 0.040), lr: 0.019900507413187767\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data(samples=1000, classes=3)\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 256, weight_regularizer_l2=5e-4, bias_regularizer_l2=5e-4)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(256, 3)\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_Adam(learning_rate=0.02, decay=5e-7)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    data_loss = loss_activation.forward(dense2.output, y)\n",
    "    # Calculate regularization penalty\n",
    "    regularization_loss = loss_activation.loss.regularization_loss(dense1) + loss_activation.loss.regularization_loss(dense2)\n",
    "    # Calculate overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "        f'acc: {accuracy:.3f}, ' +\n",
    "        f'loss: {loss:.3f} (' +\n",
    "        f'data_loss: {data_loss:.3f}, ' +\n",
    "        f'reg_loss: {regularization_loss:.3f}), ' +\n",
    "        f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81f45838-b807-4e8f-b831-bfb3e44f1660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation, acc: 0.880, loss: 0.303\n"
     ]
    }
   ],
   "source": [
    "# Validate the model\n",
    "# Create test dataset\n",
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "# Perform a forward pass of our testing data through this layer\n",
    "dense1.forward(X_test)\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "# Perform a forward pass through the activation/loss function\n",
    "# takes the output of second dense layer here and returns loss\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predictions==y_test)\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "811f005f-5545-4125-ad31-e3a526302ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.290, loss: 1.099 (data_loss: 1.099, reg_loss: 0.000), lr: 0.05\n",
      "epoch: 100, acc: 0.556, loss: 0.913 (data_loss: 0.887, reg_loss: 0.026), lr: 0.04975371909050202\n",
      "epoch: 200, acc: 0.611, loss: 0.849 (data_loss: 0.816, reg_loss: 0.033), lr: 0.049507401356502806\n",
      "epoch: 300, acc: 0.631, loss: 0.828 (data_loss: 0.795, reg_loss: 0.033), lr: 0.0492635105177595\n",
      "epoch: 400, acc: 0.617, loss: 0.831 (data_loss: 0.800, reg_loss: 0.031), lr: 0.04902201088288642\n",
      "epoch: 500, acc: 0.647, loss: 0.827 (data_loss: 0.795, reg_loss: 0.031), lr: 0.048782867456949125\n",
      "epoch: 600, acc: 0.638, loss: 0.805 (data_loss: 0.774, reg_loss: 0.030), lr: 0.04854604592455945\n",
      "epoch: 700, acc: 0.651, loss: 0.792 (data_loss: 0.763, reg_loss: 0.029), lr: 0.048311512633460556\n",
      "epoch: 800, acc: 0.622, loss: 0.801 (data_loss: 0.773, reg_loss: 0.028), lr: 0.04807923457858551\n",
      "epoch: 900, acc: 0.662, loss: 0.795 (data_loss: 0.767, reg_loss: 0.028), lr: 0.04784917938657352\n",
      "epoch: 1000, acc: 0.654, loss: 0.786 (data_loss: 0.758, reg_loss: 0.027), lr: 0.04762131530072861\n",
      "epoch: 1100, acc: 0.629, loss: 0.790 (data_loss: 0.763, reg_loss: 0.026), lr: 0.04739561116640599\n",
      "epoch: 1200, acc: 0.656, loss: 0.788 (data_loss: 0.762, reg_loss: 0.026), lr: 0.04717203641681212\n",
      "epoch: 1300, acc: 0.623, loss: 0.808 (data_loss: 0.783, reg_loss: 0.025), lr: 0.04695056105920466\n",
      "epoch: 1400, acc: 0.637, loss: 0.810 (data_loss: 0.786, reg_loss: 0.024), lr: 0.04673115566147951\n",
      "epoch: 1500, acc: 0.626, loss: 0.766 (data_loss: 0.742, reg_loss: 0.024), lr: 0.046513791339132055\n",
      "epoch: 1600, acc: 0.643, loss: 0.773 (data_loss: 0.749, reg_loss: 0.024), lr: 0.04629843974258068\n",
      "epoch: 1700, acc: 0.619, loss: 0.799 (data_loss: 0.775, reg_loss: 0.024), lr: 0.046085073044840774\n",
      "epoch: 1800, acc: 0.634, loss: 0.784 (data_loss: 0.760, reg_loss: 0.024), lr: 0.04587366392953806\n",
      "epoch: 1900, acc: 0.645, loss: 0.764 (data_loss: 0.741, reg_loss: 0.024), lr: 0.04566418557925019\n",
      "epoch: 2000, acc: 0.619, loss: 0.784 (data_loss: 0.761, reg_loss: 0.023), lr: 0.045456611664166556\n",
      "epoch: 2100, acc: 0.655, loss: 0.765 (data_loss: 0.742, reg_loss: 0.023), lr: 0.045250916331055706\n",
      "epoch: 2200, acc: 0.649, loss: 0.772 (data_loss: 0.749, reg_loss: 0.023), lr: 0.0450470741925312\n",
      "epoch: 2300, acc: 0.653, loss: 0.774 (data_loss: 0.752, reg_loss: 0.022), lr: 0.04484506031660612\n",
      "epoch: 2400, acc: 0.656, loss: 0.745 (data_loss: 0.723, reg_loss: 0.022), lr: 0.04464485021652753\n",
      "epoch: 2500, acc: 0.661, loss: 0.758 (data_loss: 0.736, reg_loss: 0.022), lr: 0.044446419840881816\n",
      "epoch: 2600, acc: 0.656, loss: 0.798 (data_loss: 0.776, reg_loss: 0.022), lr: 0.04424974556396301\n",
      "epoch: 2700, acc: 0.663, loss: 0.746 (data_loss: 0.724, reg_loss: 0.022), lr: 0.04405480417639544\n",
      "epoch: 2800, acc: 0.659, loss: 0.756 (data_loss: 0.734, reg_loss: 0.022), lr: 0.04386157287600334\n",
      "epoch: 2900, acc: 0.663, loss: 0.795 (data_loss: 0.773, reg_loss: 0.022), lr: 0.04367002925891961\n",
      "epoch: 3000, acc: 0.636, loss: 0.773 (data_loss: 0.751, reg_loss: 0.022), lr: 0.043480151310926564\n",
      "epoch: 3100, acc: 0.658, loss: 0.752 (data_loss: 0.731, reg_loss: 0.022), lr: 0.04329191739902161\n",
      "epoch: 3200, acc: 0.646, loss: 0.785 (data_loss: 0.763, reg_loss: 0.021), lr: 0.043105306263201\n",
      "epoch: 3300, acc: 0.649, loss: 0.758 (data_loss: 0.737, reg_loss: 0.021), lr: 0.0429202970084553\n",
      "epoch: 3400, acc: 0.635, loss: 0.747 (data_loss: 0.726, reg_loss: 0.021), lr: 0.04273686909696996\n",
      "epoch: 3500, acc: 0.638, loss: 0.746 (data_loss: 0.725, reg_loss: 0.021), lr: 0.04255500234052514\n",
      "epoch: 3600, acc: 0.653, loss: 0.774 (data_loss: 0.754, reg_loss: 0.021), lr: 0.042374676893088686\n",
      "epoch: 3700, acc: 0.651, loss: 0.746 (data_loss: 0.726, reg_loss: 0.020), lr: 0.042195873243596776\n",
      "epoch: 3800, acc: 0.665, loss: 0.742 (data_loss: 0.722, reg_loss: 0.020), lr: 0.04201857220891634\n",
      "epoch: 3900, acc: 0.649, loss: 0.741 (data_loss: 0.720, reg_loss: 0.020), lr: 0.041842754926984395\n",
      "epoch: 4000, acc: 0.660, loss: 0.742 (data_loss: 0.723, reg_loss: 0.020), lr: 0.04166840285011875\n",
      "epoch: 4100, acc: 0.658, loss: 0.756 (data_loss: 0.736, reg_loss: 0.020), lr: 0.041495497738495375\n",
      "epoch: 4200, acc: 0.654, loss: 0.745 (data_loss: 0.725, reg_loss: 0.020), lr: 0.041324021653787346\n",
      "epoch: 4300, acc: 0.669, loss: 0.744 (data_loss: 0.725, reg_loss: 0.019), lr: 0.041153956952961035\n",
      "epoch: 4400, acc: 0.660, loss: 0.744 (data_loss: 0.724, reg_loss: 0.019), lr: 0.040985286282224684\n",
      "epoch: 4500, acc: 0.645, loss: 0.735 (data_loss: 0.716, reg_loss: 0.019), lr: 0.04081799257112535\n",
      "epoch: 4600, acc: 0.649, loss: 0.748 (data_loss: 0.729, reg_loss: 0.019), lr: 0.04065205902678971\n",
      "epoch: 4700, acc: 0.673, loss: 0.755 (data_loss: 0.736, reg_loss: 0.019), lr: 0.04048746912830479\n",
      "epoch: 4800, acc: 0.658, loss: 0.778 (data_loss: 0.759, reg_loss: 0.019), lr: 0.04032420662123473\n",
      "epoch: 4900, acc: 0.656, loss: 0.725 (data_loss: 0.706, reg_loss: 0.019), lr: 0.04016225551226957\n",
      "epoch: 5000, acc: 0.651, loss: 0.737 (data_loss: 0.718, reg_loss: 0.019), lr: 0.04000160006400256\n",
      "epoch: 5100, acc: 0.674, loss: 0.781 (data_loss: 0.763, reg_loss: 0.018), lr: 0.039842224789832265\n",
      "epoch: 5200, acc: 0.643, loss: 0.733 (data_loss: 0.715, reg_loss: 0.018), lr: 0.03968411444898608\n",
      "epoch: 5300, acc: 0.653, loss: 0.758 (data_loss: 0.740, reg_loss: 0.018), lr: 0.03952725404166173\n",
      "epoch: 5400, acc: 0.661, loss: 0.730 (data_loss: 0.712, reg_loss: 0.018), lr: 0.03937162880428363\n",
      "epoch: 5500, acc: 0.646, loss: 0.768 (data_loss: 0.750, reg_loss: 0.018), lr: 0.03921722420487078\n",
      "epoch: 5600, acc: 0.656, loss: 0.738 (data_loss: 0.720, reg_loss: 0.018), lr: 0.03906402593851323\n",
      "epoch: 5700, acc: 0.666, loss: 0.730 (data_loss: 0.712, reg_loss: 0.018), lr: 0.038912019922954205\n",
      "epoch: 5800, acc: 0.658, loss: 0.748 (data_loss: 0.730, reg_loss: 0.018), lr: 0.038761192294274965\n",
      "epoch: 5900, acc: 0.655, loss: 0.768 (data_loss: 0.751, reg_loss: 0.018), lr: 0.038611529402679645\n",
      "epoch: 6000, acc: 0.645, loss: 0.743 (data_loss: 0.725, reg_loss: 0.018), lr: 0.03846301780837725\n",
      "epoch: 6100, acc: 0.666, loss: 0.709 (data_loss: 0.691, reg_loss: 0.018), lr: 0.03831564427755853\n",
      "epoch: 6200, acc: 0.665, loss: 0.780 (data_loss: 0.762, reg_loss: 0.018), lr: 0.03816939577846483\n",
      "epoch: 6300, acc: 0.664, loss: 0.736 (data_loss: 0.719, reg_loss: 0.018), lr: 0.038024259477546674\n",
      "epoch: 6400, acc: 0.657, loss: 0.745 (data_loss: 0.727, reg_loss: 0.018), lr: 0.03788022273570969\n",
      "epoch: 6500, acc: 0.656, loss: 0.750 (data_loss: 0.732, reg_loss: 0.018), lr: 0.03773727310464546\n",
      "epoch: 6600, acc: 0.667, loss: 0.761 (data_loss: 0.743, reg_loss: 0.018), lr: 0.03759539832324524\n",
      "epoch: 6700, acc: 0.672, loss: 0.747 (data_loss: 0.729, reg_loss: 0.017), lr: 0.03745458631409416\n",
      "epoch: 6800, acc: 0.641, loss: 0.747 (data_loss: 0.730, reg_loss: 0.018), lr: 0.03731482518004403\n",
      "epoch: 6900, acc: 0.660, loss: 0.730 (data_loss: 0.712, reg_loss: 0.017), lr: 0.03717610320086248\n",
      "epoch: 7000, acc: 0.668, loss: 0.743 (data_loss: 0.726, reg_loss: 0.017), lr: 0.03703840882995667\n",
      "epoch: 7100, acc: 0.660, loss: 0.733 (data_loss: 0.716, reg_loss: 0.017), lr: 0.036901730691169414\n",
      "epoch: 7200, acc: 0.664, loss: 0.734 (data_loss: 0.717, reg_loss: 0.017), lr: 0.03676605757564617\n",
      "epoch: 7300, acc: 0.671, loss: 0.746 (data_loss: 0.730, reg_loss: 0.017), lr: 0.03663137843877066\n",
      "epoch: 7400, acc: 0.638, loss: 0.739 (data_loss: 0.722, reg_loss: 0.017), lr: 0.03649768239716778\n",
      "epoch: 7500, acc: 0.670, loss: 0.706 (data_loss: 0.689, reg_loss: 0.017), lr: 0.03636495872577185\n",
      "epoch: 7600, acc: 0.658, loss: 0.738 (data_loss: 0.721, reg_loss: 0.017), lr: 0.03623319685495851\n",
      "epoch: 7700, acc: 0.671, loss: 0.734 (data_loss: 0.718, reg_loss: 0.016), lr: 0.03610238636773891\n",
      "epoch: 7800, acc: 0.651, loss: 0.744 (data_loss: 0.727, reg_loss: 0.017), lr: 0.03597251699701428\n",
      "epoch: 7900, acc: 0.653, loss: 0.731 (data_loss: 0.714, reg_loss: 0.016), lr: 0.035843578622889706\n",
      "epoch: 8000, acc: 0.656, loss: 0.726 (data_loss: 0.710, reg_loss: 0.017), lr: 0.03571556127004536\n",
      "epoch: 8100, acc: 0.668, loss: 0.733 (data_loss: 0.716, reg_loss: 0.017), lr: 0.03558845510516389\n",
      "epoch: 8200, acc: 0.665, loss: 0.716 (data_loss: 0.700, reg_loss: 0.017), lr: 0.03546225043441257\n",
      "epoch: 8300, acc: 0.659, loss: 0.743 (data_loss: 0.726, reg_loss: 0.017), lr: 0.035336937700978836\n",
      "epoch: 8400, acc: 0.664, loss: 0.771 (data_loss: 0.754, reg_loss: 0.017), lr: 0.03521250748265784\n",
      "epoch: 8500, acc: 0.670, loss: 0.736 (data_loss: 0.719, reg_loss: 0.017), lr: 0.035088950489490865\n",
      "epoch: 8600, acc: 0.667, loss: 0.739 (data_loss: 0.722, reg_loss: 0.017), lr: 0.0349662575614532\n",
      "epoch: 8700, acc: 0.659, loss: 0.735 (data_loss: 0.717, reg_loss: 0.017), lr: 0.034844419666190465\n",
      "epoch: 8800, acc: 0.656, loss: 0.734 (data_loss: 0.717, reg_loss: 0.017), lr: 0.034723427896801974\n",
      "epoch: 8900, acc: 0.685, loss: 0.740 (data_loss: 0.723, reg_loss: 0.017), lr: 0.03460327346967023\n",
      "epoch: 9000, acc: 0.671, loss: 0.725 (data_loss: 0.708, reg_loss: 0.017), lr: 0.034483947722335255\n",
      "epoch: 9100, acc: 0.668, loss: 0.748 (data_loss: 0.731, reg_loss: 0.017), lr: 0.034365442111412764\n",
      "epoch: 9200, acc: 0.653, loss: 0.738 (data_loss: 0.721, reg_loss: 0.017), lr: 0.03424774821055516\n",
      "epoch: 9300, acc: 0.675, loss: 0.735 (data_loss: 0.719, reg_loss: 0.017), lr: 0.03413085770845422\n",
      "epoch: 9400, acc: 0.661, loss: 0.713 (data_loss: 0.696, reg_loss: 0.017), lr: 0.034014762406884586\n",
      "epoch: 9500, acc: 0.657, loss: 0.739 (data_loss: 0.722, reg_loss: 0.017), lr: 0.03389945421878708\n",
      "epoch: 9600, acc: 0.666, loss: 0.713 (data_loss: 0.696, reg_loss: 0.017), lr: 0.033784925166390756\n",
      "epoch: 9700, acc: 0.646, loss: 0.738 (data_loss: 0.721, reg_loss: 0.017), lr: 0.03367116737937304\n",
      "epoch: 9800, acc: 0.658, loss: 0.735 (data_loss: 0.718, reg_loss: 0.017), lr: 0.033558173093056816\n",
      "epoch: 9900, acc: 0.657, loss: 0.721 (data_loss: 0.704, reg_loss: 0.017), lr: 0.0334459346466437\n",
      "epoch: 10000, acc: 0.661, loss: 0.727 (data_loss: 0.710, reg_loss: 0.017), lr: 0.03333444448148271\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data(samples=1000, classes=3)\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64, weight_regularizer_l2=5e-4,\n",
    "bias_regularizer_l2=5e-4)\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "# Create dropout layer\n",
    "dropout1 = Layer_Dropout(0.1)\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_Adam(learning_rate=0.05, decay=5e-5)\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "    # Perform a forward pass through Dropout layer\n",
    "    dropout1.forward(activation1.output)\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(dropout1.output)\n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    data_loss = loss_activation.forward(dense2.output, y)\n",
    "    # Calculate regularization penalty\n",
    "    regularization_loss = \\\n",
    "    loss_activation.loss.regularization_loss(dense1) + \\\n",
    "    loss_activation.loss.regularization_loss(dense2)\n",
    "    # Calculate overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "        f'acc: {accuracy:.3f}, ' +\n",
    "        f'loss: {loss:.3f} (' +\n",
    "        f'data_loss: {data_loss:.3f}, ' +\n",
    "        f'reg_loss: {regularization_loss:.3f}), ' +\n",
    "        f'lr: {optimizer.current_learning_rate}')\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    dropout1.backward(dense2.dinputs)\n",
    "    activation1.backward(dropout1.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e13040c-8fc6-408a-860d-a96ef60a6e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation, acc: 0.710, loss: 0.632\n"
     ]
    }
   ],
   "source": [
    "# Validate the model\n",
    "# Create test dataset\n",
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "# Perform a forward pass of our testing data through this layer\n",
    "dense1.forward(X_test)\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "# Perform a forward pass through the activation/loss function\n",
    "# takes the output of second dense layer here and returns loss\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predictions==y_test)\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19aa7dc-0c96-456a-a5c6-44ca0538f069",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
